{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Environment Setup </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package installs\n",
    "#E:\\Python310\\python.exe -m pip install --upgrade pip\n",
    "#!E:\\Python310\\Scripts\\pip3.exe install moviepy pydub SpeechRecognition pyAudioAnalysis speechbrain pyannote.audio praat-parselmouth\n",
    "#!E:\\Python310\\python.exe -m pip install git+https://github.com/openai/whisper.git soundfile\n",
    "\n",
    "!pip install --upgrade pip\n",
    "!pip install moviepy pydub SpeechRecognition pyAudioAnalysis speechbrain pyannote.audio praat-parselmouth\n",
    "!pip -m pip install git+https://github.com/openai/whisper.git soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norms for data storage and manipulation\n",
    "\n",
    "#ENV_FOLDER_DATA = source mp4 folder -> this is where the mp4s might be downloaded\n",
    "## This should be separate so we can iterate through this easily\n",
    "#ENV_FOLDER_DATA_PROC = where folders w/ processed + temp data will live\n",
    "\n",
    "## raw per-episode data storage\n",
    "#ENV_FOLDER_DATA_PROC\\\\eps\\\\#### = 3-4 digit numbered folder which represents the episode number\n",
    "#ENV_FOLDER_DATA_PROC\\\\eps\\\\####\\\\####.mp3 = saved mp3\n",
    "#ENV_FOLDER_DATA_PROC\\\\eps\\\\####\\\\####.wav = saved wav file\n",
    "#ENV_FOLDER_DATA_PROC\\\\eps\\\\####\\\\wavsplit\\\\ = folders w/ split wav files\n",
    "#ENV_FOLDER_DATA_PROC\\\\eps\\\\####\\\\wavsplit\\\\segment#-speaker#.wav = split wav file\n",
    "\n",
    "## shared outputs from process\n",
    "#ENV_FOLDER_DATA_PROC\\\\pickle\\\\####.pickle = saved metadata about the files, processing, locations, etc. all stored as a dictionary in a pickle\n",
    "#ENV_FOLDER_DATA_PROC\\\\segmentation\\\\####.txt = saved speaker segmentation from diarization\n",
    "#ENV_FOLDER_DATA_PROC\\\\speakers.json = json which identifies Joe Rogan vs Other Speaker\n",
    "## 568|SPEAKER 0|Joe Rogan\n",
    "## 568|SPEAKER 1|Rhonda Patrick\n",
    "#ENV_FOLDER_DATA_PROC\\\\targetclips\\\\####.txt = chosen clips for use in analysis\n",
    "## 10\n",
    "## 11\n",
    "## 12\n",
    "#ENV_FOLDER_DATA_PROC\\\\pratt\\\\####-segment#-speaker#.pickle = saved pratt data in dictionaries with the 4 time series outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV_HOSTNAME:The-Beast\n",
      "ENV_FOLDER_DATA:C:\\Users\\jakes\\Documents\\COMS 4732 - Computer Vision\\W4732CompVisFinal\\Data\\\n",
      "ENV_PRATT:\n"
     ]
    }
   ],
   "source": [
    "#Global variables\n",
    "import socket\n",
    "import os\n",
    "ENV_HOSTNAME = socket.gethostname()\n",
    "print('ENV_HOSTNAME:' + ENV_HOSTNAME)\n",
    "\n",
    "#store defaults for Jacob here:\n",
    "ENV_FOLDER_DATA = 'C:\\\\Users\\\\jakes\\\\Documents\\\\COMS 4732 - Computer Vision\\\\W4732CompVisFinal\\\\Data\\\\'\n",
    "ENV_FOLDER_DATA_PROC = 'C:\\\\Users\\\\jakes\\\\Documents\\\\COMS 4732 - Computer Vision\\\\W4732CompVisFinal\\\\Data_Proc\\\\'\n",
    "ENV_PRATT = ''\n",
    "\n",
    "\n",
    "if ENV_HOSTNAME == 'JWGamingPC':\n",
    "    ENV_FOLDER_DATA = 'E:\\\\W4732 Computer Vision\\\\Final Paper Data\\\\'\n",
    "    ENV_FOLDER_DATA_PROC = 'E:\\\\W4732 Computer Vision\\\\Final Paper Data Proc\\\\'\n",
    "    ENV_PRATT = ''\n",
    "\n",
    "print('ENV_FOLDER_DATA:' + ENV_FOLDER_DATA)\n",
    "print('ENV_PRATT:' + ENV_PRATT)\n",
    "\n",
    "# Create folder structure\n",
    "import os\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'segmentation', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'targetdf', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'pratt', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'eps', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Functions </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Audio Processing 1 ##\n",
    "# Code to generate mp3s, wavs, and segmentation\n",
    "# Norm - save all filepaths as \"suffixes\" aka filesuf and always concatenate the ENV_FOLDER_DATA or the ENV_FOLDER_DATA_PROC\n",
    "\n",
    "#Utility Function\n",
    "## 1) Store all the metadata et al from the functions into a dictionary, which then gets saved to a blob\n",
    "## 2) Save and load blob\n",
    "## 3) Figure out names of relevant files and relevant folder structure\n",
    "## 4) Download episodes from archive.org\n",
    "\n",
    "#Audio Functions\n",
    "## 1) Split MP4 to MP3\n",
    "## 2) MP3 to WAV\n",
    "## 3) WAV to speaker identification and time splits + record file\n",
    "## 4) WAV splits into individual files\n",
    "## 5) Figure out which segments to analyze w/ video (skip first and last segment from the speaker)\n",
    "## 6) Take first second of the segment and produce Pratt time series (0.1 second intervals)\n",
    "## 6a+b+c+d) Pitch + Intensity + Harmonics + Jitter \n",
    "\n",
    "#Library imports\n",
    "import moviepy\n",
    "import moviepy.editor\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Pipeline\n",
    "import csv\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "#extract audio into mp3\n",
    "#https://medium.com/featurepreneur/extracting-audio-from-video-using-pythons-moviepy-library-e351cd652ab8\n",
    "\n",
    "def split_mp4_to_mp3(filepath_mp4 , filepath_mp3):\n",
    "    # Load the video clip\n",
    "    video_clip = moviepy.editor.VideoFileClip(filepath_mp4)\n",
    "\n",
    "    # Extract the audio from the video clip\n",
    "    audio_clip = video_clip.audio\n",
    "\n",
    "    # Write the audio to a separate file\n",
    "    audio_clip.write_audiofile(filepath_mp3)\n",
    "\n",
    "    # Close the video and audio clips\n",
    "    audio_clip.close()\n",
    "    video_clip.close()\n",
    "\n",
    "def split_mp3_to_wav(filepath_mp3, filepath_wav):\n",
    "    #read mp3\n",
    "    mp3_clip = AudioSegment.from_mp3(filepath_mp3)\n",
    "    mp3_clip.export(filepath_wav, format=\"wav\")\n",
    "    del mp3_clip\n",
    "\n",
    "def speaker_diarization(filepath_wav,filepath_segmentation):\n",
    "    #perform speaker diarization (lingo for \"speaker recognition\")\n",
    "    #https://medium.com/@gil.shomron/whos-talking-speaker-diarization-and-emotion-recognition-in-radio-3e9623baeb2c\n",
    "\n",
    "    pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization-3.1',\n",
    "                                        use_auth_token='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')\n",
    "    \n",
    "    pipeline.to(torch.device(\"cuda\"))\n",
    "    diarization = pipeline(filepath_wav)\n",
    "    #for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    #  print('start={.1f} stop={.1f} speaker_{}'.format(turn.start,\n",
    "    #                                                   turn.end,\n",
    "    #                                                   speaker))    \n",
    "\n",
    "    # Dump to file in an RTTM format\n",
    "    #with open(ENV_FOLDER_DATA_PROC + '568\\\\segment.txt', 'w') as rttm:\n",
    "    #    diarization.write_rttm(rttm)\n",
    "    list_diarization_data = []\n",
    "    i = 0\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        list_temp = []\n",
    "        list_temp.append(str(i))\n",
    "        list_temp.append(str(speaker))\n",
    "        list_temp.append(str(turn.start))\n",
    "        list_temp.append(str(turn.end))\n",
    "        list_diarization_data.append(list_temp)\n",
    "        i += 1\n",
    "    with open(filepath_segmentation, \"w\", newline='\\n') as f:\n",
    "        writer = csv.writer(f, delimiter='|',  quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerows(list_diarization_data)\n",
    "    #took 81 minutes\n",
    "\n",
    "def wav_file_splitting(filepath_wav, filepath_segmentation,folderpath_wavsplit):\n",
    "    #use cutoffs to split wav file into sections\n",
    "    #https://stackoverflow.com/questions/51622865/break-up-a-wav-file-by-timestamp\n",
    "\n",
    "    listdict_data = []\n",
    "    #read csv file\n",
    "    with open(filepath_segmentation, newline='\\n') as csv_file:\n",
    "        reader = csv.reader(csv_file, delimiter='|')\n",
    "        for row in reader:\n",
    "            dict_temp = {}\n",
    "            dict_temp['str_segment'] = row[0]\n",
    "            dict_temp['speaker'] = row[1]\n",
    "            dict_temp['sec_start'] = row[2]\n",
    "            dict_temp['sec_end'] = row[3]\n",
    "            listdict_data.append(dict_temp)\n",
    "    \n",
    "    #split \n",
    "    audio = AudioSegment.from_wav(filepath_wav)\n",
    "    for  idx,dict_data in enumerate(listdict_data):\n",
    "        start = int(float(dict_data['sec_start'])*1000)  #pydub works in millisec\n",
    "        end = int(float(dict_data['sec_end']) * 1000) #pydub works in millisec\n",
    "        audio_chunk=audio[start:end]\n",
    "        audio_chunk.export( folderpath_wavsplit + dict_data['str_segment'] + '-' + dict_data['speaker'] + \".wav\", format=\"wav\")\n",
    "\n",
    "\n",
    "def process_mp4s_for_processing(filesuf_mp4, recalc = False):\n",
    "    #get number after # but before space afterwards\n",
    "    str_epnum_temp = filesuf_mp4.split('#')[1]\n",
    "    str_epnum = str_epnum_temp.split(' ')[0]\n",
    "    \n",
    "    #generate filepaths\n",
    "    filepath_mp4 = ENV_FOLDER_DATA + filesuf_mp4\n",
    "    print('Episode='+ str_epnum + ' at ' + filepath_mp4)\n",
    "    folderpath_eps = ENV_FOLDER_DATA_PROC + 'eps\\\\' + str_epnum + '\\\\'\n",
    "    os.makedirs(folderpath_eps, exist_ok=True)\n",
    "    filepath_mp3 = folderpath_eps + str_epnum + '.mp3'\n",
    "    filepath_wav = folderpath_eps + str_epnum + '.wav'\n",
    "    folderpath_wavsplit = folderpath_eps + 'wavsplit\\\\'\n",
    "    os.makedirs(folderpath_wavsplit, exist_ok=True)\n",
    "    #filepath_pickle = ENV_FOLDER_DATA_PROC + 'pickle\\\\' + str_epnum + '.pickle'\n",
    "    filepath_segmentation = ENV_FOLDER_DATA_PROC + 'segmentation\\\\' + str_epnum + '.psv'\n",
    "    #filepath_targetdf = ENV_FOLDER_DATA_PROC + 'targetdf\\\\' + str_epnum + '.pickle'\n",
    "    #filepath_pratt = ENV_FOLDER_DATA_PROC + 'pratt\\\\' + str_epnum + '.pickle'\n",
    "\n",
    "    #check if mp3 exists - if it doesn't, create it\n",
    "    if not os.path.exists(filepath_mp3):\n",
    "        split_mp4_to_mp3(filepath_mp4,filepath_mp3)\n",
    "    if not os.path.exists(filepath_mp3):\n",
    "        print('Failed to create MP3:'+ filepath_mp3)\n",
    "        return -1\n",
    "    #check if wav exists - if it doesn't, create it\n",
    "    if not os.path.exists(filepath_wav):\n",
    "        split_mp3_to_wav(filepath_mp3,filepath_wav)\n",
    "    #fail if process fails to produce the expected output\n",
    "    if not os.path.exists(filepath_wav):\n",
    "        print('Failed to create WAV:'+ filepath_wav)\n",
    "        return -1\n",
    "    \n",
    "    #check if segmentation exists - if it doesn't, create it\n",
    "    if not os.path.exists(filepath_segmentation):\n",
    "        speaker_diarization(filepath_wav,filepath_segmentation)\n",
    "    else:\n",
    "        print(\"Speaker diarization exists:\" + filepath_segmentation)\n",
    "    #fail if process fails to produce the expected output\n",
    "    if not os.path.exists(filepath_segmentation):\n",
    "        print('Failed to create segmentation:'+ filepath_segmentation)\n",
    "        return -1\n",
    "    \n",
    "    #check if split wav files exist - if it doesn't, create it\n",
    "    if len(glob.glob(folderpath_wavsplit + '*')) < 10:\n",
    "        wav_file_splitting(filepath_wav, filepath_segmentation,folderpath_wavsplit)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Audio Processing 2 ##\n",
    "# Code to generate target clips, pratt pickles\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "from pydub import AudioSegment\n",
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "#AudioSegment.converter = \"C:\\\\ProgramData\\\\chocolatey\\\\lib\\\\ffmpeg-full\\\\tools\\\\ffmpeg\\\\bin\\\\ffmpeg.exe\"\n",
    "#AudioSegment.ffmpeg = \"C:\\\\ProgramData\\\\chocolatey\\\\lib\\\\ffmpeg-full\\\\tools\\\\ffmpeg\\\\bin\\\\ffmpeg.exe\"\n",
    "#AudioSegment.ffprobe =\"C:\\\\ProgramData\\\\chocolatey\\\\lib\\\\ffmpeg-full\\\\tools\\\\ffmpeg\\\\bin\\\\ffprobe.exe\"\n",
    "\n",
    "def process_mp4s_for_analysis(filesuf_mp4, recalc = False):\n",
    "    #get number after # but before space afterwards\n",
    "    str_epnum_temp = filesuf_mp4.split('#')[1]\n",
    "    str_epnum = str_epnum_temp.split(' ')[0]\n",
    "    \n",
    "    #generate filepaths\n",
    "    filepath_mp4 = ENV_FOLDER_DATA + filesuf_mp4\n",
    "    print('Episode='+ str_epnum + ' at ' + filepath_mp4)\n",
    "    folderpath_eps = ENV_FOLDER_DATA_PROC + 'eps\\\\' + str_epnum + '\\\\'\n",
    "    os.makedirs(folderpath_eps, exist_ok=True)\n",
    "    filepath_mp3 = folderpath_eps + str_epnum + '.mp3'\n",
    "    filepath_wav = folderpath_eps + str_epnum + '.wav'\n",
    "    folderpath_wavsplit = folderpath_eps + 'wavsplit\\\\'\n",
    "    os.makedirs(folderpath_wavsplit, exist_ok=True)\n",
    "    filepath_segmentation = ENV_FOLDER_DATA_PROC + 'segmentation\\\\' + str_epnum + '.psv'\n",
    "    #filepath_targetdf = ENV_FOLDER_DATA_PROC + 'targetdf\\\\' + str_epnum + '.pickle'\n",
    "    filepath_pratt = ENV_FOLDER_DATA_PROC + 'pratt\\\\' + str_epnum + '.pickle'\n",
    "\n",
    "    if os.path.exists(filepath_pratt):\n",
    "        print('Pratt data already generated:'+ filepath_pratt)\n",
    "        return 0\n",
    "\n",
    "    dict_speakerdata = {}\n",
    "    for i in range(20):\n",
    "        speaker = 'SPEAKER_' + str(i).zfill(2)\n",
    "        dict_speakerdata[speaker] = {}\n",
    "        dict_speakerdata[speaker]['list_pitch'] = []\n",
    "        dict_speakerdata[speaker]['list_intensity'] = []\n",
    "        dict_speakerdata[speaker]['list_shimmer'] = []\n",
    "        dict_speakerdata[speaker]['list_jitter'] = []\n",
    "        dict_speakerdata[speaker]['list_harmonics'] = []\n",
    "\n",
    "\n",
    "    #get list of wav files to iterate through\n",
    "    dictdict_output = {} #key = filesuf / value = dictionary\n",
    "    for path_wav in glob.glob(folderpath_wavsplit + '*.wav'):\n",
    "        #create data points and save into a dictionary\n",
    "        dict_temp = {}\n",
    "        dict_temp['path'] = path_wav\n",
    "        filesuf = os.path.basename(path_wav)\n",
    "        dict_temp['filesuf'] = filesuf\n",
    "        dict_temp['str_segment'] = filesuf.split('-')[0]\n",
    "        speaker = (filesuf.split('-')[1]).split('.')[0]\n",
    "        dict_temp['speaker'] = speaker\n",
    "        \n",
    "\n",
    "        sound_total = parselmouth.Sound(path_wav)\n",
    "        second_duration = call(sound_total, \"Get total duration\") \n",
    "        tenth_seconds = int(second_duration * 10)\n",
    "        dict_temp['duration'] = second_duration\n",
    "        #create dictionaries for all the values to be stored\n",
    "        list_pitch = []\n",
    "        list_intensity = []\n",
    "        list_shimmer = []\n",
    "        list_jitter = []\n",
    "        list_harmonics = []\n",
    "\n",
    "        for t in range(tenth_seconds):\n",
    "\n",
    "            start_time = t * 0.1\n",
    "            end_time = (t + 1) * 0.1\n",
    "            sound = sound_total.extract_part(from_time=start_time, to_time=end_time)\n",
    "\n",
    "            #iterate through the parts of the sound\n",
    "\n",
    "            pointprocess = call(sound, \"To PointProcess (periodic, cc)\",75, 600)\n",
    "            #dict_temp['pointprocess'] = pointprocess\n",
    "            \n",
    "            #https://parselmouth.readthedocs.io/_/downloads/en/stable/pdf/\n",
    "            #gets the pitch , and sets the pitch floor to 75 and tge outcg max to 600\n",
    "            try:\n",
    "                pitch = call(sound, \"To Pitch\", 0.0, 75, 600)\n",
    "                val_pitch = call(pitch, \"Get mean\", 0, 0,\"Hertz\")\n",
    "            except:\n",
    "                val_pitch = None\n",
    "            \n",
    "            ##-\tFor intensity extraction, set the pitch floor to 100Hz. Use ‘energy’ averaging method to get mean intensity.\n",
    "            try:\n",
    "                intensity = call(sound, \"To Intensity\", 100,0.01)\n",
    "                val_intensity = call(intensity, \"Get mean\", 0, 0,\"energy\")\n",
    "            except:\n",
    "                val_intensity = None\n",
    "\n",
    "            ##Shimmer\n",
    "            # For shimmer, extract local shimmer only, and set period floor to 0.0001s, period ceiling to 0.02s, maximum period factor to 1.3, and maximum amplitude factor to 1.6.\n",
    "            try:\n",
    "                val_shimmer = call([sound, pointprocess], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "            except:\n",
    "                val_shimmer = None\n",
    "            \n",
    "            # For jitter, extract local jitter only, and set period floor to 0.0001s, period ceiling to 0.02s, and maximum period factor to 1.3\n",
    "            #Please convert from a Sound object to a PointProcess (periodic, cc) object. (#74)\n",
    "            #https://github.com/drfeinberg/PraatScripts/blob/master/Measure%20Pitch%2C%20HNR%2C%20Jitter%2C%20Shimmer%2C%20and%20Formants.ipynb\n",
    "            #f0min , f0max\n",
    "            try:\n",
    "                val_jitter = call(pointprocess, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "            except:\n",
    "                val_jitter = None\n",
    "            \n",
    "            #-\tTo calculate HNR (harmonics-to-noise ratio), extract harmonicity (cc) first. Set time step to 0.01, minimum pitch to 75Hz, silence threshold to 0.1, and number of periods per window to 1.0.\n",
    "            try:\n",
    "                harmonics = call(sound, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "                val_harmonics = call(harmonics, \"Get mean\", 0, 0)\n",
    "            except:\n",
    "                val_harmonics = None\n",
    "\n",
    "            #add to relevant lists\n",
    "            list_pitch.append(val_pitch)\n",
    "            list_intensity.append(val_intensity)\n",
    "            list_shimmer.append(val_shimmer)\n",
    "            list_jitter.append(val_jitter)\n",
    "            list_harmonics.append(val_harmonics)\n",
    "\n",
    "            dict_speakerdata[speaker]['list_pitch'].append(val_pitch)\n",
    "            dict_speakerdata[speaker]['list_intensity'].append(val_intensity)\n",
    "            dict_speakerdata[speaker]['list_shimmer'].append(val_shimmer)\n",
    "            dict_speakerdata[speaker]['list_jitter'].append(val_jitter)\n",
    "            dict_speakerdata[speaker]['list_harmonics'].append(val_harmonics)\n",
    "        #end for loop that goes per-0.1 second\n",
    "        dict_temp['list_pitch'] = list_pitch\n",
    "        dict_temp['list_intensity'] = list_intensity\n",
    "        dict_temp['list_shimmer'] = list_shimmer\n",
    "        dict_temp['list_jitter'] = list_jitter\n",
    "        dict_temp['list_harmonics'] = list_harmonics\n",
    "        \n",
    "        dictdict_output[filesuf] = dict_temp\n",
    "    #end for loop that goes through each file in the folder\n",
    "    \n",
    "    #calculate statistics for the speakers\n",
    "    for i in range(20):\n",
    "        speaker = 'SPEAKER_' + str(i).zfill(2)\n",
    "        dictdict_output[speaker] = {}\n",
    "        print(dictdict_output)\n",
    "\n",
    "        dictdict_output[speaker]['mean_pitch'] = np.nanmean(list(filter(None, dict_speakerdata[speaker]['list_pitch']) ))\n",
    "        dictdict_output[speaker]['median_pitch'] = np.nanmedian(list(filter(None,dict_speakerdata[speaker]['list_pitch'])))\n",
    "        dictdict_output[speaker]['mean_intensity'] = np.nanmean(list(filter(None,dict_speakerdata[speaker]['list_intensity'])))\n",
    "        dictdict_output[speaker]['median_intensity'] = np.nanmedian(list(filter(None,dict_speakerdata[speaker]['list_intensity'])))\n",
    "        dictdict_output[speaker]['mean_shimmer'] = np.nanmean(list(filter(None,dict_speakerdata[speaker]['list_shimmer'])))\n",
    "        dictdict_output[speaker]['median_shimmer'] = np.nanmedian(list(filter(None,dict_speakerdata[speaker]['list_shimmer'])))\n",
    "        dictdict_output[speaker]['mean_jitter'] = np.nanmean(list(filter(None,dict_speakerdata[speaker]['list_jitter'])))\n",
    "        dictdict_output[speaker]['median_jitter'] = np.nanmedian(list(filter(None,dict_speakerdata[speaker]['list_jitter'])))\n",
    "        dictdict_output[speaker]['mean_harmonics'] = np.nanmean(list(filter(None,dict_speakerdata[speaker]['list_harmonics'])))\n",
    "        dictdict_output[speaker]['median_harmonics'] = np.nanmedian(list(filter(None,dict_speakerdata[speaker]['list_harmonics'])))\n",
    "\n",
    "\n",
    "\n",
    "    #save data\n",
    "    with open(filepath_pratt, 'wb') as file:\n",
    "        print(filepath_pratt)\n",
    "        pickle.dump(dictdict_output, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Audio Processing 3 ##\n",
    "# Producing a filtered object to use for training the model\n",
    "# Target output is a dataframe saved into the targetdf folder\n",
    "# data will be filtering out both speaker = Joe Rogan and speaker = Misc so only the guest is included\n",
    "# data will also be filtering out the first instance of talking by that speaker\n",
    "# data will also be filtering out clips < 1 second\n",
    "# Dataframe has the following columns\n",
    "# epnum -> int (episode #)\n",
    "# seg -> int (segment)\n",
    "# type -> p for pitch, i for intensity,  j for fitter , h for harmonics, s for shimmer \n",
    "# m -> global median for that statistic\n",
    "# d0,d1, .... d99 -> values for the first 10 seconds in 0.1 second increments \n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import copy\n",
    "\n",
    "def process_analysis_for_model(filesuf_mp4, recalc = False):\n",
    "    #get number after # but before space afterwards\n",
    "    str_epnum_temp = filesuf_mp4.split('#')[1]\n",
    "    str_epnum = str_epnum_temp.split(' ')[0]\n",
    "    int_epnum = int(str_epnum)\n",
    "    \n",
    "    #generate filepaths\n",
    "    filepath_mp4 = ENV_FOLDER_DATA + filesuf_mp4\n",
    "    print('Episode='+ str_epnum + ' at ' + filepath_mp4)\n",
    "    folderpath_eps = ENV_FOLDER_DATA_PROC + 'eps\\\\' + str_epnum + '\\\\'\n",
    "    os.makedirs(folderpath_eps, exist_ok=True)\n",
    "    filepath_mp3 = folderpath_eps + str_epnum + '.mp3'\n",
    "    filepath_wav = folderpath_eps + str_epnum + '.wav'\n",
    "    folderpath_wavsplit = folderpath_eps + 'wavsplit\\\\'\n",
    "    os.makedirs(folderpath_wavsplit, exist_ok=True)\n",
    "    filepath_segmentation = ENV_FOLDER_DATA_PROC + 'segmentation\\\\' + str_epnum + '.psv'\n",
    "    filepath_targetdf = ENV_FOLDER_DATA_PROC + 'targetdf\\\\' + str_epnum + '.pickle'\n",
    "    filepath_pratt = ENV_FOLDER_DATA_PROC + 'pratt\\\\' + str_epnum + '.pickle'\n",
    "    filepath_speakers = ENV_FOLDER_DATA_PROC + 'speakers.json'\n",
    "\n",
    "    #open dict of speakers\n",
    "    \n",
    "    json_speakers = {}\n",
    "    with open(filepath_speakers) as f:\n",
    "        json_speakers = json.load(f)\n",
    "    \n",
    "    #determine speakers to retain\n",
    "    list_speakers_keep = []\n",
    "\n",
    "    for key,val in json_speakers[str_epnum].items():\n",
    "        if val == 'Joe Rogan':\n",
    "            continue\n",
    "        if val == 'Misc':\n",
    "            continue\n",
    "        list_speakers_keep.append(key)\n",
    "\n",
    "    #declare default dictionary\n",
    "    dict_default = {}\n",
    "    dict_default['epnum'] = int_epnum\n",
    "    dict_default['seg'] = 0\n",
    "    dict_default['type'] = ''\n",
    "    dict_default['m'] = 0\n",
    "    for i in range(100):\n",
    "        dict_default['d' + str(i)] = 0\n",
    "\n",
    "    #declare empty dataframe with the 44 columns\n",
    "    df = pd.DataFrame( columns= list(dict_default.keys()) )\n",
    "\n",
    "    #open up pratt pickle\n",
    "    dictdict_output = None\n",
    "    with open(filepath_pratt, 'rb') as file:\n",
    "        print(filepath_pratt)\n",
    "        dictdict_output = pickle.load(file)\n",
    "    \n",
    "    #iterate through each segment\n",
    "    for key,val in dictdict_output.items():\n",
    "        #if there's no dash in the key, then the key is one of the aggregate values\n",
    "        if '-' not in key:\n",
    "            continue\n",
    "        filesuf = val['filesuf']\n",
    "        str_segment = val['str_segment']\n",
    "        int_segment = int(str_segment)\n",
    "        speaker = val['speaker']\n",
    "\n",
    "        #skip if it's less than 1 second\n",
    "        if val['duration'] < 1.0:\n",
    "            continue\n",
    "\n",
    "        #skip data if it's Joe Rogan or Misc\n",
    "        if speaker not in list_speakers_keep:\n",
    "            continue\n",
    "\n",
    "        dict_temp = copy.deepcopy(dict_default)\n",
    "        dict_temp['seg'] = int_segment\n",
    "        list_dict = []\n",
    "\n",
    "        #make a new copy for pitch, intensity, shimmer, jitter, harmonics\n",
    "        #pitch\n",
    "        dict_pitch = copy.deepcopy(dict_temp)\n",
    "        dict_pitch['type'] = 'p'\n",
    "        dict_pitch['m'] = dictdict_output[speaker]['median_pitch']\n",
    "        for i, data in enumerate(val['list_pitch']):\n",
    "            dict_pitch['d'+str(i)] = data\n",
    "        list_dict.append(dict_pitch)\n",
    "\n",
    "\n",
    "        #intensity\n",
    "        dict_intensity = copy.deepcopy(dict_temp)\n",
    "        dict_intensity['type'] = 'i'\n",
    "        dict_intensity['m'] = dictdict_output[speaker]['median_intensity']\n",
    "        for i, data in enumerate(val['list_intensity']):\n",
    "            dict_intensity['d'+str(i)] = data\n",
    "        list_dict.append(dict_intensity)\n",
    "\n",
    "\n",
    "        #shimmer\n",
    "        dict_shimmer = copy.deepcopy(dict_temp)\n",
    "        dict_shimmer['type'] = 's'\n",
    "        dict_shimmer['m'] = dictdict_output[speaker]['median_shimmer']\n",
    "        for i, data in enumerate(val['list_shimmer']):\n",
    "            dict_shimmer['d'+str(i)] = data\n",
    "        list_dict.append(dict_shimmer)\n",
    "\n",
    "\n",
    "        #jitter\n",
    "        dict_jitter = copy.deepcopy(dict_temp)\n",
    "        dict_jitter['type'] = 'j'\n",
    "        dict_jitter['m'] = dictdict_output[speaker]['median_jitter']\n",
    "        for i, data in enumerate(val['list_jitter']):\n",
    "            dict_jitter['d'+str(i)] = data\n",
    "        list_dict.append(dict_jitter)\n",
    "\n",
    "\n",
    "        #harmonics\n",
    "        dict_harmonics = copy.deepcopy(dict_temp)\n",
    "        dict_harmonics['type'] = 'h'\n",
    "        dict_harmonics['m'] = dictdict_output[speaker]['median_harmonics']\n",
    "        for i, data in enumerate(val['list_harmonics']):\n",
    "            dict_harmonics['d'+str(i)] = data\n",
    "        list_dict.append(dict_harmonics)\n",
    "\n",
    "        #turn list of dicts into dataframe\n",
    "        df_temp = pd.DataFrame(list_dict, columns=list(dict_default.keys()) )\n",
    "        df = pd.concat([df,df_temp], ignore_index=True)\n",
    "    \n",
    "    #end loop through data\n",
    "    \n",
    "    #save data\n",
    "    with open(filepath_targetdf, 'wb') as file:\n",
    "        pickle.dump(df, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test audio processing\n",
    "# using test video #568 Dr. Rhonda Patrick\n",
    "# https://archive.org/download/jre-001-837/JRE_001-837/\n",
    "# https://archive.org/download/jre-001-837/JRE_001-837/Joe%20Rogan%20Experience%20%23568%20-%20Dr.%20Rhonda%20Patrick.mp4\n",
    "\n",
    "filesuf_mp4 = 'Joe Rogan Experience #568 - Dr. Rhonda Patrick.mp4'\n",
    "#process_mp4s_for_processing(filesuf_mp4)\n",
    "\n",
    "  #speaker 2 is joe rogan\n",
    "  #speaker 0 is the lady\n",
    "  #speaker 1 is the sound effects \n",
    "  #time start and stop is in seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pratt processing\n",
    "filesuf_mp4 = 'Joe Rogan Experience #568 - Dr. Rhonda Patrick.mp4'\n",
    "#process_mp4s_for_analysis(filesuf_mp4, recalc = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataframe output\n",
    "filesuf_mp4 = 'Joe Rogan Experience #568 - Dr. Rhonda Patrick.mp4'\n",
    "#process_analysis_for_model(filesuf_mp4, recalc = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import unquote\n",
    "import urllib.request\n",
    "#download videos from archive.org\n",
    "\n",
    "#get files in https://archive.org/download/jre-001-837/JRE_001-837/\n",
    "#code from https://www.geeksforgeeks.org/extract-all-the-urls-from-the-webpage-using-python/\n",
    "url = 'https://archive.org/download/jre-001-837/JRE_001-837/'\n",
    "reqs = requests.get(url)\n",
    "soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "urls = []\n",
    "for link in soup.find_all('a'):\n",
    "    urls.append(link.get('href'))\n",
    "\n",
    "#download files that only have a single person (no ampersand or comma) that isn't a fight companion and also between 200 and 700\n",
    "#also, no partial episodes\n",
    "list_dlurl = []\n",
    "for url in urls:\n",
    "    if url is None:\n",
    "        continue\n",
    "    #allow PHD\n",
    "    str_temp = url.replace('%2C%20PhD','')\n",
    "    if not '.mp4' in str_temp:\n",
    "        continue\n",
    "    if '.ia.mp4' in str_temp:\n",
    "        continue\n",
    "    if 'Part' in str_temp: #remove multi-part episodes\n",
    "        continue\n",
    "    if '%2C' in str_temp: #remove comma\n",
    "        continue\n",
    "    if '%26' in str_temp: #remove ampersand\n",
    "        continue\n",
    "    if 'McAfee' in str_temp: #McAfee interview conducted over phone\n",
    "        continue\n",
    "\n",
    "    \n",
    "    str_epnum = str_temp.split('%23')[1]\n",
    "    str_epnum = str_epnum.split('%20')[0]\n",
    "    int_epnum = int(str_epnum)\n",
    "    #they up the file size after 595\n",
    "    if int_epnum < 200 or int_epnum > 595:\n",
    "        continue\n",
    "    list_dlurl.append(url)\n",
    "\n",
    "for dlurl in list_dlurl:\n",
    "    # check if file already exists in downloads folder before downloading\n",
    "    filename = unquote(dlurl)\n",
    "    if os.path.exists(ENV_FOLDER_DATA + filename):\n",
    "        print('File already exists:' + filename)\n",
    "        continue\n",
    "    url_final = 'https://archive.org/download/jre-001-837/JRE_001-837/' + dlurl\n",
    "    print('Downloading:' + filename)\n",
    "    urllib.request.urlretrieve(url_final, ENV_FOLDER_DATA + filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "for path_mp4 in glob.glob(ENV_FOLDER_DATA + '*.mp4'):\n",
    "    filesuf_mp4 = os.path.basename(path_mp4)\n",
    "    process_mp4s_for_processing(filesuf_mp4)\n",
    "    process_mp4s_for_analysis(filesuf_mp4, recalc = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode=568 at C:\\Users\\jakes\\Documents\\COMS 4732 - Computer Vision\\W4732CompVisFinal\\Data\\Joe Rogan Experience #568 - Dr. Rhonda Patrick.mp4\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\jakes\\\\Documents\\\\COMS 4732 - Computer Vision\\\\W4732CompVisFinal\\\\Data_Proc\\\\speakers.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#need to fill out speakers first\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mprocess_analysis_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilesuf_mp4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecalc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 41\u001b[0m, in \u001b[0;36mprocess_analysis_for_model\u001b[1;34m(filesuf_mp4, recalc)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#open dict of speakers\u001b[39;00m\n\u001b[0;32m     40\u001b[0m json_speakers \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath_speakers\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     42\u001b[0m     json_speakers \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#determine speakers to retain\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:308\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m     )\n\u001b[1;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\jakes\\\\Documents\\\\COMS 4732 - Computer Vision\\\\W4732CompVisFinal\\\\Data_Proc\\\\speakers.json'"
     ]
    }
   ],
   "source": [
    "#need to fill out speakers first\n",
    "process_analysis_for_model(filesuf_mp4, recalc = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lower the resolution to 360p\n",
    "2. Split the video to match the speaker diarization\n",
    "3. Seperating the Video (Picture Frames) (gif?) from the mp4 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ffmpeg-python\n",
    "\n",
    "import ffmpeg\n",
    "\n",
    "input_file = ENV_FOLDER_DATA  + 'Joe Rogan Experience #568 - Dr. Rhonda Patrick.mp4'\n",
    "\n",
    "def get_video_resolution(video_file):\n",
    "    probe = ffmpeg.probe(video_file)\n",
    "    video_stream = next((stream for stream in probe['streams'] if stream['codec_type'] == 'video'), None)\n",
    "    if video_stream:\n",
    "        width = int(video_stream['width'])\n",
    "        height = int(video_stream['height'])\n",
    "        resolution = (width, height)\n",
    "        return resolution\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def is_360p(video_file):\n",
    "    resolution = get_video_resolution(video_file)\n",
    "    if resolution:\n",
    "        width, height = resolution\n",
    "        if width <= 640 and height <= 360:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def convert_to_360p(video_file, output_file):\n",
    "    ffmpeg.input(video_file).output(output_file, s='640x360').run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_rows(filename):\n",
    "    merged_rows = []\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        current_speaker = None\n",
    "        start_time = None\n",
    "        end_time = None\n",
    "\n",
    "        for line in file:\n",
    "            parts = line.strip().split('|')\n",
    "            speaker = parts[1]\n",
    "            start = float(parts[2])\n",
    "            end = float(parts[3])\n",
    "\n",
    "            if speaker != current_speaker:\n",
    "                if current_speaker is not None:\n",
    "                    merged_rows.append((current_speaker, start_time, end_time))\n",
    "                current_speaker = speaker\n",
    "                start_time = start\n",
    "                end_time = end\n",
    "            else:\n",
    "                end_time = end\n",
    "\n",
    "        # Append the last speaker\n",
    "        if current_speaker is not None:\n",
    "            merged_rows.append((current_speaker, start_time, end_time))\n",
    "\n",
    "    return merged_rows\n",
    "\n",
    "# Example usage\n",
    "# filename = ENV_FOLDER_DATA_PROC + 'segmentation\\\\208.psv'\n",
    "# merged_data = merge_rows(filename)\n",
    "# for row in merged_data:\n",
    "#     if row[2] - row[1] > 30:\n",
    "#         print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_clip_times_from_psv(psv_file):\n",
    "    clips = []\n",
    "    merge_data = merge_rows(psv_file)\n",
    "    for speaker, start_time, end_time in merge_data:\n",
    "        if end_time - start_time > 20 and  end_time - start_time < 40:\n",
    "            clips.append((start_time, end_time))\n",
    "    return clips\n",
    "\n",
    "def extract_clips(input_file, output_prefix, clips):\n",
    "    for i, (start_time, end_time) in enumerate(clips, start=1):\n",
    "        start_time_1dec = round(start_time, 1)\n",
    "        end_time_1dec = round(end_time, 1)\n",
    "        output_file = f\"{output_prefix}_clip{start_time_1dec}-{end_time_1dec}.gif\"\n",
    "        ffmpeg.input(input_file, ss=start_time, to=end_time).output(output_file).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psv_filename = ENV_FOLDER_DATA_PROC + 'segmentation\\\\208.psv'\n",
    "# input_mp4 = ENV_FOLDER_DATA + 'Joe Rogan Experience #208 - Freeway Rick Ross.mp4'\n",
    "# output_gif = ENV_FOLDER_DATA_PROC + 'targetclips\\\\208'\n",
    "\n",
    "# clips = get_clip_times_from_psv(psv_filename)\n",
    "# extract_clips(input_mp4, output_gif, clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "\n",
    "def detect_scene_changes(gif_path, threshold=10):\n",
    "    gif = cv2.VideoCapture(gif_path)\n",
    "    frame_rate = gif.get(cv2.CAP_PROP_FPS) # Get frame rate\n",
    "    prev_frame = None\n",
    "    scene_changes = []\n",
    "\n",
    "    frame_number = 0\n",
    "    while True:\n",
    "        ret, frame = gif.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert frame to grayscale for simplicity\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if prev_frame is not None:\n",
    "            # Calculate frame difference\n",
    "            frame_diff = cv2.absdiff(gray_frame, prev_frame)\n",
    "            diff_score = frame_diff.mean()\n",
    "            \n",
    "            # Check if scene change detected\n",
    "            if diff_score > threshold:\n",
    "                # Calculate time in seconds\n",
    "                time_in_seconds = frame_number / frame_rate\n",
    "                scene_changes.append((time_in_seconds, diff_score))\n",
    "        \n",
    "        prev_frame = gray_frame\n",
    "        frame_number += 1\n",
    "\n",
    "    return scene_changes\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# for file in glob.glob(ENV_FOLDER_DATA_PROC + 'targetclips\\\\*.gif'):\n",
    "#     scene_changes = detect_scene_changes(file)\n",
    "#     print(\"Scene changes for\", file)\n",
    "#     for change in scene_changes:\n",
    "#         print(\"Time:\", change[0], \"seconds, Score:\", change[1])\n",
    "# gif_path = ENV_FOLDER_DATA_PROC + \"targetclips\\\\208_clip1479.3-1502.9.gif\"\n",
    "# scene_changes = detect_scene_changes(gif_path)\n",
    "# print(\"Scene changes:\")\n",
    "# for change in scene_changes:\n",
    "#     print(\"Time:\", change[0], \"seconds, Score:\", change[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete clips where scene changes are detected unless the scene changes within the first 5 seconds or the last 5 seconds\n",
    "\n",
    "def get_clip_length(filename):\n",
    "    # Extract start and end timestamps from filename\n",
    "    start, end = filename.split(\"_clip\")[1].split(\".gif\")[0].split(\"-\")\n",
    "    \n",
    "    # Convert timestamps to float\n",
    "    start = float(start)\n",
    "    end = float(end)\n",
    "    \n",
    "    # Calculate clip length\n",
    "    clip_length = end - start\n",
    "    \n",
    "    return clip_length\n",
    "\n",
    "def delete_clips_with_scene_changes(gif_path, threshold=10):\n",
    "\n",
    "    for file in glob.glob(gif_path):\n",
    "        scene_changes = detect_scene_changes(file)\n",
    "        if len(scene_changes) == 0:\n",
    "            continue\n",
    "\n",
    "        clip_length = get_clip_length(file)\n",
    "        \n",
    "        first_change = scene_changes[0]\n",
    "        last_change = scene_changes[-1]\n",
    "\n",
    "        if len(scene_changes) < 2:\n",
    "            if first_change[0] < 5 or last_change[0] > clip_length - 5:\n",
    "                #print(\"Scene change detected within first or last 5 seconds\")\n",
    "                continue\n",
    "\n",
    "        #print(\"Deleting\", file)\n",
    "        os.remove(file)\n",
    "\n",
    "# Example usage\n",
    "#delete_clips_with_scene_changes(ENV_FOLDER_DATA_PROC + \"targetclips\\\\*.gif\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_vids(Data_dir):\n",
    "    \n",
    "    for file in glob.glob(Data_dir + '/*.mp4'):\n",
    "        episode_number = re.search(r'#(\\d+)', file).group(1)\n",
    "\n",
    "        # if vid not 360p, convert to 360p\n",
    "\n",
    "        if not is_360p(file):\n",
    "            convert_to_360p(file, file)\n",
    "\n",
    "        psv_filename = ENV_FOLDER_DATA_PROC + 'segmentation\\\\' + episode_number + '.psv'\n",
    "        input_mp4 = file\n",
    "        \n",
    "        #make directory for target clips for each episode\n",
    "\n",
    "        if not os.path.exists(ENV_FOLDER_DATA_PROC + 'targetclips\\\\' + episode_number):\n",
    "            os.mkdir(ENV_FOLDER_DATA_PROC + 'targetclips\\\\' + episode_number)\n",
    "\n",
    "        output_gif = ENV_FOLDER_DATA_PROC + 'targetclips\\\\' + episode_number + '\\\\' + episode_number\n",
    "\n",
    "        clips = get_clip_times_from_psv(psv_filename)\n",
    "        extract_clips(input_mp4, output_gif, clips)\n",
    "\n",
    "        delete_clips_with_scene_changes(ENV_FOLDER_DATA_PROC + \"targetclips\\\\\" + episode_number + \"\\\\*.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\jakes\\\\Documents\\\\COMS 4732 - Computer Vision\\\\W4732CompVisFinal\\\\Data_Proc\\\\segmentation\\\\256.psv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msplit_vids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mENV_FOLDER_DATA\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m, in \u001b[0;36msplit_vids\u001b[1;34m(Data_dir)\u001b[0m\n\u001b[0;32m     18\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(ENV_FOLDER_DATA_PROC \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargetclips\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m episode_number)\n\u001b[0;32m     20\u001b[0m output_gif \u001b[38;5;241m=\u001b[39m ENV_FOLDER_DATA_PROC \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargetclips\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m episode_number \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m episode_number\n\u001b[1;32m---> 22\u001b[0m clips \u001b[38;5;241m=\u001b[39m \u001b[43mget_clip_times_from_psv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpsv_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m extract_clips(input_mp4, output_gif, clips)\n\u001b[0;32m     25\u001b[0m delete_clips_with_scene_changes(ENV_FOLDER_DATA_PROC \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargetclips\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m episode_number \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m*.gif\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m, in \u001b[0;36mget_clip_times_from_psv\u001b[1;34m(psv_file)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_clip_times_from_psv\u001b[39m(psv_file):\n\u001b[0;32m      4\u001b[0m     clips \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m     merge_data \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpsv_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m speaker, start_time, end_time \u001b[38;5;129;01min\u001b[39;00m merge_data:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m end_time \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m  end_time \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m40\u001b[39m:\n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36mmerge_rows\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_rows\u001b[39m(filename):\n\u001b[0;32m      2\u001b[0m     merged_rows \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      5\u001b[0m         current_speaker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         start_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:308\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m     )\n\u001b[1;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\jakes\\\\Documents\\\\COMS 4732 - Computer Vision\\\\W4732CompVisFinal\\\\Data_Proc\\\\segmentation\\\\256.psv'"
     ]
    }
   ],
   "source": [
    "split_vids(ENV_FOLDER_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Appendix </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Appendix 1##\n",
    "# Failed Speaker Diarization 1\n",
    "%%script echo skipping appendix\n",
    "\n",
    "#https://medium.com/@apparaomulpuri/speaker-diarization-in-python-a-step-by-step-guide-351a094237f2\n",
    "#perform speaker diarization (lingo for \"speaker recognition\")\n",
    "#this is a poorly performing solution\n",
    "\n",
    "import librosa #after further analysis librosa is actually a music library - seems cool\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def generate_speaker_labels(filepath_wav):\n",
    "    audio, sr = librosa.load(filepath_wav, sr=None)\n",
    "    duration = librosa.get_duration(y=audio, sr=sr)\n",
    "    #print('Duration:' + str(duration))\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    mfccs_scaled = scaler.fit_transform(mfccs.T)\n",
    "    kmeans = KMeans(n_clusters=3)  # Adjust based on the expected number of speakers\n",
    "    speaker_labels = kmeans.fit_predict(mfccs_scaled)\n",
    "\n",
    "    del audio\n",
    "    del sr\n",
    "    del mfccs\n",
    "    del scaler\n",
    "    del mfccs_scaled\n",
    "    del kmeans\n",
    "\n",
    "    return speaker_labels, duration\n",
    "    \n",
    "list_speakers , duration = generate_speaker_labels(filepath_testwav)\n",
    "\n",
    "#print(len(list_speakers))\n",
    "# 917952\n",
    "# sample rate = 22050\n",
    "# hop length = 512\n",
    "#print(str( len(list_speakers) / duration))\n",
    "#86.13282789423312 <- samples per second\n",
    "#print(str((60*60*2) + (60 * 57) + 39))\n",
    "\n",
    "#now that we have categorizations, let's perform cutoffs to split the speech (and video) between the speakers:\n",
    "samples_per_sec = 1.0 * len(list_speakers) / duration\n",
    "list_cutoffs = []\n",
    "current_speaker = list_speakers[0]\n",
    "temp_dict = {}\n",
    "temp_dict['speaker'] = current_speaker\n",
    "temp_dict['start_index'] = 0\n",
    "for i,speaker in enumerate(list_speakers):\n",
    "    if speaker == temp_dict['speaker'] and (i != (len(list_speakers) - 1)):\n",
    "        continue\n",
    "    temp_dict['end_index'] = i - 1\n",
    "    list_cutoffs.append(temp_dict)\n",
    "    temp_dict = {}\n",
    "    temp_dict['speaker'] = speaker\n",
    "    temp_dict['start_index'] = i\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "audio = AudioSegment.from_wav(filepath_testwav)\n",
    "\n",
    "\n",
    "for  idx,dict_tim in enumerate(list_cutoffs):\n",
    "    start = int((dict_tim['start_index'] / samples_per_sec) * 1000) #pydub works in millisec\n",
    "    end = int((dict_tim['end_index'] / samples_per_sec) * 1000) #pydub works in millisec\n",
    "    audio_chunk=audio[start:end]\n",
    "    audio_chunk.export( ENV_FOLDER_DATA_PROC + '568\\\\' + str(end) + '-'  + str(dict_tim['speaker'])  + \".wav\", format=\"wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Appendix 2##\n",
    "# Failed Speaker Diarization 2\n",
    "%%script echo skipping appendix\n",
    "\n",
    "#perform speaker diarization (lingo for \"speaker recognition\")\n",
    "#attempt 2\n",
    "#This is using old code thus will not run\n",
    "#https://picovoice.ai/blog/speaker-diarization-in-python/\n",
    "#https://speechbrain.github.io/ \n",
    "#https://colab.research.google.com/drive/1nMKHOTTROwQitOXQEYq35lvv7nyTOlpe?usp=sharing\n",
    "from simple_diarizer.diarizer import Diarizer\n",
    "\n",
    "diar = Diarizer(\n",
    "        embed_model='ecapa', # supported types: ['xvec', 'ecapa']\n",
    "        cluster_method='sc', # supported types: ['ahc', 'sc']\n",
    "        window=1.5, # size of window to extract embeddings (in seconds)\n",
    "        period=0.75 # hop of window (in seconds)\n",
    "    )\n",
    "segments = diar.diarize(filepath_testwav, \n",
    "                        num_speakers=None,\n",
    "                        threshold=1e-1,\n",
    "                        outfile=ENV_FOLDER_DATA_PROC + '568\\\\segment.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appendix 3 - code to transcribe text\n",
    "\n",
    "# Import the required libraries\n",
    "import speech_recognition as sr  # Library for speech recognition\n",
    "import os  # Library for interacting with the operating system\n",
    "from pydub import AudioSegment  # Library for working with audio files\n",
    "from pydub.silence import split_on_silence  # Function for splitting audio files based on silence\n",
    "\n",
    "#https://stackoverflow.com/questions/65489705/transcribing-mp3-to-text-python-riff-id-error\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def transcribe_large_audio_file(prefix,path_mp3):\n",
    "    \"\"\"\n",
    "    Split audio into chunks and apply speech recognition\n",
    "    \"\"\"\n",
    "    # Load audio file with pydub\n",
    "    audio = AudioSegment.from_mp3(path_mp3)\n",
    "    # Split audio at silent parts with duration of 700ms or more and obtain chunks\n",
    "    audio_chunks = split_on_silence(audio, min_silence_len=600, silence_thresh=audio.dBFS-14, keep_silence=600)\n",
    "\n",
    "    # Create a directory to store audio chunks\n",
    "    chunks_dir = ENV_FOLDER_DATA_PROC + prefix\n",
    "    if not os.path.isdir(chunks_dir):\n",
    "        os.mkdir(chunks_dir)\n",
    "\n",
    "    full_text = \"\"\n",
    "    failed_attempts = 0\n",
    "    # Process each audio chunk\n",
    "    for i, chunk in enumerate(audio_chunks, start=1):\n",
    "        # Save chunk in the directory\n",
    "        chunk_file_name = os.path.join(chunks_dir, f\"chunk{i}.wav\")\n",
    "        chunk.export(chunk_file_name, format=\"wav\")\n",
    "        # Recognize audio from the chunk\n",
    "        with sr.WavFile(chunk_file_name) as src:\n",
    "            listened_audio = recognizer.listen(src)\n",
    "            # Convert audio to text\n",
    "            try:\n",
    "                text = recognizer.recognize_whisper(listened_audio)\n",
    "            except Exception  as e:\n",
    "                failed_attempts += 1\n",
    "                print(e)\n",
    "            else:\n",
    "                failed_attempts = 0\n",
    "                text = f\"{text.capitalize()}. \"\n",
    "                print(chunk_file_name, \":\", text)\n",
    "                full_text += text\n",
    "    # Return the transcription for all chunks\n",
    "    return full_text\n",
    "\n",
    "def split_and_transcribe(prefix,filepath_mp3):\n",
    "    # Define the output directory\n",
    "    output_dir = ENV_FOLDER_DATA_PROC + prefix\n",
    "\n",
    "    # Create the output directory if it does not exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate through all .mp3 files in the directory and transcribe them\n",
    "    with open(os.path.join(output_dir, '568.txt'), 'w') as result_file:\n",
    "        print(f\"Processing {filepath_mp3}\")\n",
    "        try:\n",
    "            # Transcribe the audio file\n",
    "            transcription = transcribe_large_audio_file(prefix,filepath_mp3)\n",
    "        except LookupError as error:\n",
    "            # If there is an error, skip the file and continue with the next one\n",
    "            print(f\"Error on {filepath_mp3} due to: {error}\")\n",
    "        # Save the transcription to a text file with the same name as the audio file\n",
    "        txt_file_path = os.path.join(output_dir, f\"{os.path.splitext(filepath_mp3)[0]}.txt\")\n",
    "        with open(txt_file_path, 'w', encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(transcription)\n",
    "        # Print the transcription and the path to the saved text file\n",
    "        print(transcription)\n",
    "        print(f\"Transcription saved to {txt_file_path}\")\n",
    "        # Save the transcription to the result\n",
    "\n",
    "\n",
    "split_and_transcribe('568',filepath_testmp3)\n",
    "#splitting file into pieces\n",
    "#https://stackoverflow.com/questions/67334379/cut-mp4-in-pieces-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
