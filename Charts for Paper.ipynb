{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script is to develop the visual outputs for our final paper\n",
    "# Create new charts folder\n",
    "# folder will have example video \n",
    "# extract video and audio into memory (example file should be short)\n",
    "# run pratt analysis on audio\n",
    "# normalize values\n",
    "# predict using saved model\n",
    "# chart original, robustl2s, and our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package installs\n",
    "#E:\\Python310\\python.exe -m pip install --upgrade pip\n",
    "#!E:\\Python310\\python.exe -m pip install git+https://github.com/openai/whisper.git soundfile\n",
    "!E:\\Python310\\Scripts\\pip3.exe install ffmpeg av\n",
    "#!E:\\Python310\\python.exe -m pip uninstall torch torchvision torchaudio -y\n",
    "!E:\\Python310\\python.exe -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV_HOSTNAME:JWGamingPC\n",
      "ENV_FOLDER_DATA:E:\\W4732 Computer Vision\\Final Paper Data\\\n",
      "ENV_PRATT:\n"
     ]
    }
   ],
   "source": [
    "#Global variables\n",
    "import socket\n",
    "import os\n",
    "ENV_HOSTNAME = socket.gethostname()\n",
    "print('ENV_HOSTNAME:' + ENV_HOSTNAME)\n",
    "\n",
    "#store defaults for Jacob here:\n",
    "ENV_FOLDER_DATA = 'C:\\\\Users\\\\jakes\\\\Documents\\\\COMS 4732 - Computer Vision\\\\W4732CompVisFinal\\\\Data\\\\'\n",
    "ENV_FOLDER_DATA_PROC = 'C:\\\\Users\\\\jakes\\\\Documents\\\\COMS 4732 - Computer Vision\\\\W4732CompVisFinal\\\\Data_Proc\\\\'\n",
    "ENV_PRATT = ''\n",
    "\n",
    "\n",
    "if ENV_HOSTNAME == 'JWGamingPC':\n",
    "    ENV_FOLDER_DATA = 'E:\\\\W4732 Computer Vision\\\\Final Paper Data\\\\'\n",
    "    ENV_FOLDER_DATA_PROC = 'E:\\\\W4732 Computer Vision\\\\Final Paper Data Proc\\\\'\n",
    "    ENV_PRATT = ''\n",
    "\n",
    "print('ENV_FOLDER_DATA:' + ENV_FOLDER_DATA)\n",
    "print('ENV_PRATT:' + ENV_PRATT)\n",
    "\n",
    "# Create folder structure\n",
    "import os\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'segmentation', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'targetdf', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'pratt', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'eps', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'clips', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'charts', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variables for the process\n",
    "#\n",
    "# p / i / s / j / h\n",
    "ANALYSIS_TYPE = 'p'\n",
    "PATH_ORIGMP4 = ''\n",
    "PATH_ROBMP4 = ''\n",
    "\n",
    "PATH_MODEL = ''\n",
    "if ANALYSIS_TYPE == 'p':\n",
    "    PATH_MODEL = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "import moviepy\n",
    "from moviepy.editor import *\n",
    "import moviepy.editor\n",
    "from pydub import AudioSegment\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "def create_chart_data(filesuf_mp4):\n",
    "\n",
    "    #save all the data into this dictionary\n",
    "    dict_results = {}\n",
    "\n",
    "    #filepaths\n",
    "    folderpath_base = ENV_FOLDER_DATA_PROC + 'charts\\\\'\n",
    "    filepath_mp4 = folderpath_base + filesuf_mp4\n",
    "    filepath_mp3 = folderpath_base + 'temp.mp3'\n",
    "    filepath_wav = folderpath_base + 'temp.wav'\n",
    "    filepath_normmp4 = folderpath_base + 'temp.mp4'\n",
    "    filepath_clip = folderpath_base + 'tempclip.mp4'\n",
    "\n",
    "    #convert mp4 to mp3\n",
    "    video_clip = moviepy.editor.VideoFileClip(filepath_mp4)\n",
    "    audio_clip = video_clip.audio\n",
    "    audio_clip.write_audiofile(filepath_mp3)\n",
    "    audio_clip.close()\n",
    "    video_clip.close()\n",
    "\n",
    "    #extract wav from mp3\n",
    "    mp3_clip = AudioSegment.from_mp3(filepath_mp3)\n",
    "    mp3_clip.export(filepath_wav, format=\"wav\")\n",
    "\n",
    "    #load wav file for pratt analysis\n",
    "    sound_total = parselmouth.Sound(filepath_wav)\n",
    "\n",
    "    #get pratt values\n",
    "    list_pitch = []\n",
    "    list_intensity = []\n",
    "    list_shimmer = []\n",
    "    list_jitter = []\n",
    "    list_harmonics = []\n",
    "\n",
    "    for t in range(40):\n",
    "\n",
    "        start_time = t * 0.1\n",
    "        end_time = (t + 1) * 0.1\n",
    "        sound = sound_total.extract_part(from_time=start_time, to_time=end_time)\n",
    "\n",
    "        #iterate through the parts of the sound\n",
    "\n",
    "        pointprocess = call(sound, \"To PointProcess (periodic, cc)\",75, 600)\n",
    "        #dict_temp['pointprocess'] = pointprocess\n",
    "        \n",
    "        #https://parselmouth.readthedocs.io/_/downloads/en/stable/pdf/\n",
    "        #gets the pitch , and sets the pitch floor to 75 and tge outcg max to 600\n",
    "        try:\n",
    "            pitch = call(sound, \"To Pitch\", 0.0, 75, 600)\n",
    "            val_pitch = call(pitch, \"Get mean\", 0, 0,\"Hertz\")\n",
    "        except:\n",
    "            val_pitch = None\n",
    "        \n",
    "        ##-\tFor intensity extraction, set the pitch floor to 100Hz. Use ‘energy’ averaging method to get mean intensity.\n",
    "        try:\n",
    "            intensity = call(sound, \"To Intensity\", 100,0.01)\n",
    "            val_intensity = call(intensity, \"Get mean\", 0, 0,\"energy\")\n",
    "        except:\n",
    "            val_intensity = None\n",
    "\n",
    "        ##Shimmer\n",
    "        # For shimmer, extract local shimmer only, and set period floor to 0.0001s, period ceiling to 0.02s, maximum period factor to 1.3, and maximum amplitude factor to 1.6.\n",
    "        try:\n",
    "            val_shimmer = call([sound, pointprocess], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "        except:\n",
    "            val_shimmer = None\n",
    "        \n",
    "        # For jitter, extract local jitter only, and set period floor to 0.0001s, period ceiling to 0.02s, and maximum period factor to 1.3\n",
    "        #Please convert from a Sound object to a PointProcess (periodic, cc) object. (#74)\n",
    "        #https://github.com/drfeinberg/PraatScripts/blob/master/Measure%20Pitch%2C%20HNR%2C%20Jitter%2C%20Shimmer%2C%20and%20Formants.ipynb\n",
    "        #f0min , f0max\n",
    "        try:\n",
    "            val_jitter = call(pointprocess, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "        except:\n",
    "            val_jitter = None\n",
    "        \n",
    "        #-\tTo calculate HNR (harmonics-to-noise ratio), extract harmonicity (cc) first. Set time step to 0.01, minimum pitch to 75Hz, silence threshold to 0.1, and number of periods per window to 1.0.\n",
    "        try:\n",
    "            harmonics = call(sound, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "            val_harmonics = call(harmonics, \"Get mean\", 0, 0)\n",
    "        except:\n",
    "            val_harmonics = None\n",
    "\n",
    "        #add to relevant lists\n",
    "        list_pitch.append(val_pitch)\n",
    "        list_intensity.append(val_intensity)\n",
    "        list_shimmer.append(val_shimmer)\n",
    "        list_jitter.append(val_jitter)\n",
    "        list_harmonics.append(val_harmonics)\n",
    "    \n",
    "    dict_results['p'] = list_pitch\n",
    "    dict_results['i'] = list_intensity\n",
    "    dict_results['s'] = list_shimmer\n",
    "    dict_results['j'] = list_jitter\n",
    "    dict_results['h'] = list_harmonics\n",
    "\n",
    "    #extract target video data\n",
    "    clip_total = VideoFileClip(filepath_mp4).without_audio().resize(height=360)\n",
    "    clip_total = clip_total.fx(vfx.blackwhite)\n",
    "    clip_total.write_videofile(filepath_normmp4, fps=10)\n",
    "    #get subclip\n",
    "    ffmpeg_extract_subclip(filepath_normmp4, 0, 4, targetname=filepath_clip)\n",
    "\n",
    "    \n",
    "\n",
    "    torchvision.set_video_backend('pyav')\n",
    "    reader = torchvision.io.VideoReader(filepath_clip, \"video\")\n",
    "    reader.set_current_stream(\"video\")\n",
    "    transform = torchvision.transforms.Grayscale()\n",
    "\n",
    "    video_frames = torch.empty(0)\n",
    "    frames = [] \n",
    "    ct = 1\n",
    "    for frame in reader:\n",
    "        tempframe = transform(frame['data'])\n",
    "        frames.append(tempframe)\n",
    "        ct += 1\n",
    "    for x in range(40):\n",
    "        frames.append( torch.empty((1,360,640), dtype=torch.int8) )\n",
    "    if len(frames) > 0:\n",
    "        video_frames = torch.stack(frames[:40], 0)\n",
    "    \n",
    "    dict_results['v'] = video_frames\n",
    "    \n",
    "    return dict_results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data for the original file\n",
    "\n",
    "#use the model to create a target median + 40 data\n",
    "\n",
    "#get the data for the robustls2\n",
    "\n",
    "#plot 2 lines - clip median, our predicted median\n",
    "#plot 3 line charts - actual, robust2lm's result, and our result\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
