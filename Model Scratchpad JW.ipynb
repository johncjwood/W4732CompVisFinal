{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package installs\n",
    "#E:\\Python310\\python.exe -m pip install --upgrade pip\n",
    "#!E:\\Python310\\python.exe -m pip install git+https://github.com/openai/whisper.git soundfile\n",
    "!E:\\Python310\\Scripts\\pip3.exe install ffmpeg av\n",
    "#!E:\\Python310\\python.exe -m pip uninstall torch torchvision torchaudio -y\n",
    "!E:\\Python310\\python.exe -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV_HOSTNAME:JWGamingPC\n",
      "ENV_FOLDER_DATA:E:\\W4732 Computer Vision\\Final Paper Data\\\n",
      "ENV_PRATT:\n"
     ]
    }
   ],
   "source": [
    "#Global variables\n",
    "import socket\n",
    "import os\n",
    "ENV_HOSTNAME = socket.gethostname()\n",
    "print('ENV_HOSTNAME:' + ENV_HOSTNAME)\n",
    "\n",
    "#store defaults for Jacob here:\n",
    "ENV_FOLDER_DATA = 'C:\\\\Users\\\\jakes\\\\Documents\\\\COMS 4732 - Computer Vision\\\\W4732CompVisFinal\\\\Data\\\\'\n",
    "ENV_FOLDER_DATA_PROC = 'C:\\\\Users\\\\jakes\\\\Documents\\\\COMS 4732 - Computer Vision\\\\W4732CompVisFinal\\\\Data_Proc\\\\'\n",
    "ENV_PRATT = ''\n",
    "\n",
    "\n",
    "if ENV_HOSTNAME == 'JWGamingPC':\n",
    "    ENV_FOLDER_DATA = 'E:\\\\W4732 Computer Vision\\\\Final Paper Data\\\\'\n",
    "    ENV_FOLDER_DATA_PROC = 'E:\\\\W4732 Computer Vision\\\\Final Paper Data Proc\\\\'\n",
    "    ENV_PRATT = ''\n",
    "\n",
    "print('ENV_FOLDER_DATA:' + ENV_FOLDER_DATA)\n",
    "print('ENV_PRATT:' + ENV_PRATT)\n",
    "\n",
    "# Create folder structure\n",
    "import os\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'segmentation', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'targetdf', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'pratt', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'eps', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'clips', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Model here\n",
    "# Need to define it up top to prevent overwriting it by mistake\n",
    "import torch\n",
    "\n",
    "class AudGenModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudGenModel, self).__init__()\n",
    "\n",
    "        chanMult = 56\n",
    "        kernel = 4\n",
    "        stride = 2\n",
    "        self.relu = torch.nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels = 1, out_channels = chanMult, kernel_size = kernel ,stride = stride ,padding = 1)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels = chanMult, out_channels = 2 * chanMult, kernel_size = kernel ,stride = stride ,padding = 1)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels = 2 * chanMult, out_channels = 4 * chanMult, kernel_size = kernel ,stride = stride ,padding = 1)\n",
    "        self.output = torch.nn.Linear(chanMult * 9 * 4, 1)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "\n",
    "        ################################\n",
    "        # Please fill in your code here:\n",
    "        ################################\n",
    "        \n",
    "        x = self.relu(self.conv1(input))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = torch.flatten(x,1) #keep the 1st dim since it's the batch\n",
    "        #x = self.relu(self.hidden(x))\n",
    "        #x = torch.functional.dropout(x,0.25)\n",
    "        x = self.output(x)\n",
    "        out = self.sigmoid(x)\n",
    "\n",
    "        \n",
    "        return out\n",
    "    \n",
    "a = AudGenModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Training 1 ##\n",
    "import pandas as pd\n",
    "import json\n",
    "import copy\n",
    "import csv\n",
    "import pickle\n",
    "import torchvision\n",
    "import itertools\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "\n",
    "#get number after # but before space afterwards\n",
    "\n",
    "#generate filepaths\n",
    "#filepath_mp4 = ENV_FOLDER_DATA + filesuf_mp4\n",
    "#folderpath_normmp4 = ENV_FOLDER_DATA_PROC + 'norm\\\\' + str_epnum + '\\\\'\n",
    "#os.makedirs(folderpath_normmp4, exist_ok=True)\n",
    "#filepath_normmp4 = folderpath_normmp4 + str_epnum + '.mp4'\n",
    "#folderpath_eps = ENV_FOLDER_DATA_PROC + 'eps\\\\' + str_epnum + '\\\\'\n",
    "#os.makedirs(folderpath_eps, exist_ok=True)\n",
    "#folderpath_clips = ENV_FOLDER_DATA_PROC + 'clips\\\\' + str_epnum + '\\\\'\n",
    "#os.makedirs(folderpath_clips, exist_ok=True)\n",
    "folderpath_clipinfo = ENV_FOLDER_DATA_PROC + 'clipinfo\\\\'\n",
    "os.makedirs(folderpath_clipinfo, exist_ok=True)\n",
    "folderpath_sourcedf = ENV_FOLDER_DATA_PROC + 'sourcedf\\\\'\n",
    "os.makedirs(folderpath_sourcedf, exist_ok=True)\n",
    "#filepath_sourcedf = folderpath_sourcedf + str_epnum + '.pickle'\n",
    "#folderpath_wavsplit = folderpath_eps + 'wavsplit\\\\'\n",
    "#os.makedirs(folderpath_wavsplit, exist_ok=True)\n",
    "#filepath_segmentation = ENV_FOLDER_DATA_PROC + 'segmentation\\\\' + str_epnum + '.psv'\n",
    "folderpath_targetdf = ENV_FOLDER_DATA_PROC + 'targetdf\\\\'\n",
    "folderpath_pratt = ENV_FOLDER_DATA_PROC + 'pratt\\\\' \n",
    "filepath_speakers = ENV_FOLDER_DATA_PROC + 'speakers.json'\n",
    "\n",
    "## get overlap between sourcedf files and targetdf files\n",
    "list_eps = []\n",
    "\n",
    "dict_sourcedf = {}\n",
    "list_sourcedf_eps = []\n",
    "for path_pickle in glob.glob(folderpath_sourcedf + '*.pickle'):\n",
    "    dict_temp = {}\n",
    "    dict_temp['path'] = path_pickle\n",
    "    str_basenm = os.path.basename(path_pickle)\n",
    "    dict_temp['basenm'] = str_basenm\n",
    "    dict_temp['str_epnum'] = str_basenm.split('.')[0]\n",
    "    int_epnum = int(dict_temp['str_epnum'])\n",
    "    dict_temp['int_epnum'] = int_epnum\n",
    "    list_sourcedf_eps.append(int_epnum)\n",
    "    dict_sourcedf[int_epnum] = dict_temp\n",
    "\n",
    "\n",
    "dict_targetdf = {}\n",
    "list_targetdf_eps = []\n",
    "for path_pickle in glob.glob(folderpath_targetdf + '*.pickle'):\n",
    "    dict_temp = {}\n",
    "    dict_temp['path'] = path_pickle\n",
    "    str_basenm = os.path.basename(path_pickle)\n",
    "    dict_temp['basenm'] = str_basenm\n",
    "    dict_temp['str_epnum'] = str_basenm.split('.')[0]\n",
    "    int_epnum = int(dict_temp['str_epnum'])\n",
    "    dict_temp['int_epnum'] = int_epnum\n",
    "    list_targetdf_eps.append(int_epnum)\n",
    "    dict_targetdf[int_epnum] = dict_temp\n",
    "\n",
    "#populate matches\n",
    "for src_ep in list_sourcedf_eps:\n",
    "    if src_ep in list_targetdf_eps:\n",
    "        list_eps.append(src_ep)\n",
    "\n",
    "\n",
    "#define model pieces\n",
    "NUM_EPOCHS = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.Adam(a.parameters(), lr=0.0008, betas=(0.65, 0.999))\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "##Epoch Loop\n",
    "for e in range(NUM_EPOCHS):\n",
    "    #make a copy of the episode list to pop out 2 at a time\n",
    "    list_epocheps = copy.deepcopy(list_eps)\n",
    "    while len(list_epocheps) > 0:\n",
    "        #load two or 3 episodes into memory at a time\n",
    "        list_tempeps = []\n",
    "        for i in range(2):\n",
    "            if len(list_epocheps) > 0:\n",
    "                list_tempeps.append(list_epocheps.pop())\n",
    "        #load pratt files into memory\n",
    "\n",
    "        #load video files into memory\n",
    "\n",
    "        #shuffle the values\n",
    "\n",
    "        #feed values into the predict / loss / optimize cycle\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = a(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        if i % 1000 == 0:\n",
    "            print(f'Epoch={epoch + 1} Iter={i + 1:5d} Loss={loss.item():.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #define a function that, given a targetdf value, look up the source - if both exists, use the data to train the model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "listdict_clips = []\n",
    "#read pickle file\n",
    "with open(filepath_targetclipinfo, 'rb') as file:\n",
    "    listdict_clips = pickle.load(file)\n",
    "\n",
    "\n",
    "#iterate through each segment\n",
    "#create the object that will be loaded into the neural network\n",
    "dict_sourceframes = {}\n",
    "for val in listdict_clips:\n",
    "\n",
    "    int_segment = val['seg']\n",
    "    sec_start = float(val['sec_start']) \n",
    "    sec_end = float(val['sec_end'])\n",
    "    filepath_clip = val['filepath_clip'] \n",
    "    duration = val['duration']\n",
    "\n",
    "    \n",
    "\n",
    "    video_frames = torch.empty(0)\n",
    "    frames = [] \n",
    "    ct = 1\n",
    "    for frame in reader:\n",
    "        tempframe = transform(frame['data'])\n",
    "        frames.append(tempframe)\n",
    "        ct += 1\n",
    "    for x in range(40):\n",
    "        frames.append( torch.empty((1,360,640), dtype=torch.int8) )\n",
    "    if len(frames) > 0:\n",
    "        video_frames = torch.stack(frames[:40], 0)\n",
    "    \n",
    "    dict_sourceframes[int_segment] = video_frames\n",
    "\n",
    "#save data\n",
    "with open(filepath_sourcedf, 'wb') as file:\n",
    "    pickle.dump(dict_sourceframes, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#end loop through data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
