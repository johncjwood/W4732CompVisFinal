{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg in e:\\python310\\lib\\site-packages (1.4)\n",
      "Requirement already satisfied: av in e:\\python310\\lib\\site-packages (12.0.0)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in e:\\python310\\lib\\site-packages (2.2.2+cu121)\n",
      "Requirement already satisfied: torchvision in e:\\python310\\lib\\site-packages (0.17.2+cu121)\n",
      "Requirement already satisfied: torchaudio in e:\\python310\\lib\\site-packages (2.2.2+cu121)\n",
      "Requirement already satisfied: filelock in e:\\python310\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in e:\\python310\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in e:\\python310\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in e:\\python310\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in e:\\python310\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in e:\\python310\\lib\\site-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: numpy in e:\\python310\\lib\\site-packages (from torchvision) (1.24.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\python310\\lib\\site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\python310\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in e:\\python310\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "#package installs\n",
    "#E:\\Python310\\python.exe -m pip install --upgrade pip\n",
    "#!E:\\Python310\\python.exe -m pip install git+https://github.com/openai/whisper.git soundfile\n",
    "!E:\\Python310\\Scripts\\pip3.exe install ffmpeg av\n",
    "#!E:\\Python310\\python.exe -m pip uninstall torch torchvision torchaudio -y\n",
    "!E:\\Python310\\python.exe -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV_HOSTNAME:JWGamingPC\n",
      "ENV_FOLDER_DATA:E:\\W4732 Computer Vision\\Final Paper Data\\\n",
      "ENV_PRATT:\n"
     ]
    }
   ],
   "source": [
    "#Global variables\n",
    "import socket\n",
    "import os\n",
    "ENV_HOSTNAME = socket.gethostname()\n",
    "print('ENV_HOSTNAME:' + ENV_HOSTNAME)\n",
    "\n",
    "#store defaults for Jacob here:\n",
    "ENV_FOLDER_DATA = 'C:\\\\Users\\\\jakes\\\\Documents\\\\COMS 4732 - Computer Vision\\\\W4732CompVisFinal\\\\Data\\\\'\n",
    "ENV_FOLDER_DATA_PROC = 'C:\\\\Users\\\\jakes\\\\Documents\\\\COMS 4732 - Computer Vision\\\\W4732CompVisFinal\\\\Data_Proc\\\\'\n",
    "ENV_PRATT = ''\n",
    "\n",
    "\n",
    "if ENV_HOSTNAME == 'JWGamingPC':\n",
    "    ENV_FOLDER_DATA = 'E:\\\\W4732 Computer Vision\\\\Final Paper Data\\\\'\n",
    "    ENV_FOLDER_DATA_PROC = 'E:\\\\W4732 Computer Vision\\\\Final Paper Data Proc\\\\'\n",
    "    ENV_PRATT = ''\n",
    "\n",
    "print('ENV_FOLDER_DATA:' + ENV_FOLDER_DATA)\n",
    "print('ENV_PRATT:' + ENV_PRATT)\n",
    "\n",
    "# Create folder structure\n",
    "import os\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'segmentation', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'targetdf', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'pratt', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'eps', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'clips', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Model here\n",
    "# Need to define it up top to prevent overwriting it by mistake\n",
    "import torch\n",
    "\n",
    "class AudGenModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudGenModel, self).__init__()\n",
    "\n",
    "        chanMult = 56\n",
    "        kernel = 4\n",
    "        stride = 2\n",
    "        num_frames = 40\n",
    "        self.relu = torch.nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv3d(in_channels = 1, out_channels = chanMult\n",
    "                                     , kernel_size = (5,40,40) ,stride = 10 ,padding = 10 \n",
    "                                     #, depth=num_frames\n",
    "                                    )\n",
    "        self.conv2 = torch.nn.Conv3d(in_channels = chanMult, out_channels = 2 * chanMult\n",
    "                                     , kernel_size = kernel ,stride = stride ,padding = 1\n",
    "                                     #, depth=num_frames\n",
    "                                     )\n",
    "        self.conv3 = torch.nn.Conv3d(in_channels = 2 * chanMult, out_channels = 4 * chanMult\n",
    "                                     , kernel_size = 3 ,stride = stride ,padding = 1\n",
    "                                     #, depth=num_frames\n",
    "                                     )\n",
    "        self.output = torch.nn.Linear(1120, 41)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "\n",
    "        ################################\n",
    "        # Please fill in your code here:\n",
    "        ################################\n",
    "        \n",
    "        x = self.relu(self.conv1(input))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = torch.flatten(x,0) #keep the 1st dim since it's the batch\n",
    "        #x = self.relu(self.hidden(x))\n",
    "        #x = torch.functional.dropout(x,0.25)\n",
    "        x = self.output(x)\n",
    "        out = self.sigmoid(x)\n",
    "\n",
    "        \n",
    "        return out\n",
    "    \n",
    "a = AudGenModel()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "a = a.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 135\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m40\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)):\n\u001b[0;32m    134\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend( torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m360\u001b[39m,\u001b[38;5;241m640\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint8) )\n\u001b[1;32m--> 135\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m#transform inputs\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m#print(inputs.size())\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m#torch.Size([40, 1, 360, 640])\u001b[39;00m\n\u001b[0;32m    139\u001b[0m inputs \u001b[38;5;241m=\u001b[39m transform(inputs\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat))\u001b[38;5;241m.\u001b[39mmovedim(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "## Model Training 1 ##\n",
    "import pandas as pd\n",
    "import json\n",
    "import copy\n",
    "import csv\n",
    "import pickle\n",
    "import torchvision\n",
    "import itertools\n",
    "import torch\n",
    "import glob\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "#get number after # but before space afterwards\n",
    "\n",
    "#generate filepaths\n",
    "#filepath_mp4 = ENV_FOLDER_DATA + filesuf_mp4\n",
    "#folderpath_normmp4 = ENV_FOLDER_DATA_PROC + 'norm\\\\' + str_epnum + '\\\\'\n",
    "#os.makedirs(folderpath_normmp4, exist_ok=True)\n",
    "#filepath_normmp4 = folderpath_normmp4 + str_epnum + '.mp4'\n",
    "#folderpath_eps = ENV_FOLDER_DATA_PROC + 'eps\\\\' + str_epnum + '\\\\'\n",
    "#os.makedirs(folderpath_eps, exist_ok=True)\n",
    "#folderpath_clips = ENV_FOLDER_DATA_PROC + 'clips\\\\' + str_epnum + '\\\\'\n",
    "#os.makedirs(folderpath_clips, exist_ok=True)\n",
    "folderpath_clipinfo = ENV_FOLDER_DATA_PROC + 'clipinfo\\\\'\n",
    "os.makedirs(folderpath_clipinfo, exist_ok=True)\n",
    "folderpath_sourcedf = ENV_FOLDER_DATA_PROC + 'sourcedf\\\\'\n",
    "os.makedirs(folderpath_sourcedf, exist_ok=True)\n",
    "#filepath_sourcedf = folderpath_sourcedf + str_epnum + '.pickle'\n",
    "#folderpath_wavsplit = folderpath_eps + 'wavsplit\\\\'\n",
    "#os.makedirs(folderpath_wavsplit, exist_ok=True)\n",
    "#filepath_segmentation = ENV_FOLDER_DATA_PROC + 'segmentation\\\\' + str_epnum + '.psv'\n",
    "folderpath_targetdf = ENV_FOLDER_DATA_PROC + 'targetdf\\\\'\n",
    "folderpath_pratt = ENV_FOLDER_DATA_PROC + 'pratt\\\\' \n",
    "filepath_speakers = ENV_FOLDER_DATA_PROC + 'speakers.json'\n",
    "\n",
    "## get overlap between sourcedf files and targetdf files\n",
    "list_eps = []\n",
    "\n",
    "#note - source is actually a dict of dict of tensors\n",
    "dict_sourcedf = {}\n",
    "list_sourcedf_eps = []\n",
    "for path_pickle in glob.glob(folderpath_sourcedf + '*.pickle'):\n",
    "    dict_temp = {}\n",
    "    dict_temp['path'] = path_pickle\n",
    "    str_basenm = os.path.basename(path_pickle)\n",
    "    dict_temp['basenm'] = str_basenm\n",
    "    dict_temp['str_epnum'] = str_basenm.split('.')[0]\n",
    "    int_epnum = int(dict_temp['str_epnum'])\n",
    "    dict_temp['int_epnum'] = int_epnum\n",
    "    list_sourcedf_eps.append(int_epnum)\n",
    "    dict_sourcedf[int_epnum] = dict_temp\n",
    "\n",
    "#note - target is actually a df\n",
    "dict_targetdf = {}\n",
    "list_targetdf_eps = []\n",
    "for path_pickle in glob.glob(folderpath_targetdf + '*.pickle'):\n",
    "    dict_temp = {}\n",
    "    dict_temp['path'] = path_pickle\n",
    "    str_basenm = os.path.basename(path_pickle)\n",
    "    dict_temp['basenm'] = str_basenm\n",
    "    dict_temp['str_epnum'] = str_basenm.split('.')[0]\n",
    "    int_epnum = int(dict_temp['str_epnum'])\n",
    "    dict_temp['int_epnum'] = int_epnum\n",
    "    list_targetdf_eps.append(int_epnum)\n",
    "    dict_targetdf[int_epnum] = dict_temp\n",
    "\n",
    "#populate matches\n",
    "for src_ep in list_sourcedf_eps:\n",
    "    if src_ep in list_targetdf_eps:\n",
    "        list_eps.append(src_ep)\n",
    "\n",
    "#40 data elements\n",
    "list_datacols = []\n",
    "list_datacols.append('m')\n",
    "for i in range(40):\n",
    "    list_datacols.append('d' + str(i))\n",
    "\n",
    "#define model pieces\n",
    "NUM_EPOCHS = 10\n",
    "TRAINING_TYPE = 'p'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform=torchvision.transforms.Normalize( mean=(255*0.5), std=(255*0.5) )\n",
    "optimizer = torch.optim.Adam(a.parameters(), lr=0.0008, betas=(0.65, 0.999))\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "\n",
    "##Epoch Loop\n",
    "for e in range(NUM_EPOCHS):\n",
    "    #make a copy of the episode list to pop out 2 at a time\n",
    "    list_epocheps = copy.deepcopy(list_eps)\n",
    "    while len(list_epocheps) > 0:\n",
    "        #load two or 3 episodes into memory at a time\n",
    "        list_tempeps = []\n",
    "        for i in range(2):\n",
    "            if len(list_epocheps) > 0:\n",
    "                list_tempeps.append(list_epocheps.pop())\n",
    "        #load pratt files into memory\n",
    "        dict_target = {}\n",
    "        for i,tempeps in enumerate(list_tempeps):\n",
    "            with open(dict_targetdf[tempeps]['path'], 'rb') as file:\n",
    "                dict_target[tempeps] =  pickle.load(file)\n",
    "                    \n",
    "        #load video files into memory\n",
    "        dict_source = {}\n",
    "        for i,tempeps in enumerate(list_tempeps):\n",
    "            with open(dict_sourcedf[tempeps]['path'], 'rb') as file:\n",
    "                dict_temp = pickle.load(file)\n",
    "                dict_source[tempeps] = dict_temp\n",
    "\n",
    "        #create a list of eps / clips and shuffle\n",
    "        listtup_epseg = []\n",
    "        for key,val in dict_source.items():\n",
    "            for key2,val2 in val.items():\n",
    "                tup_epseg = [key,key2]\n",
    "                listtup_epseg.append( tup_epseg)\n",
    "        random.shuffle(tup_epseg)\n",
    "\n",
    "        #loop for data elements\n",
    "        for tup_epseg in listtup_epseg:\n",
    "            int_epnum = tup_epseg[0]\n",
    "            int_segnum = tup_epseg[1]\n",
    "            #feed values into the predict / loss / optimize cycle\n",
    "            inputs = None\n",
    "            labels = None\n",
    "            if int_epnum not in dict_source.keys():\n",
    "                continue\n",
    "            if int_segnum not in dict_source[int_epnum].keys():\n",
    "                continue\n",
    "            inputs = dict_source[int_epnum][int_segnum]\n",
    "            #pad inputs\n",
    "            for x in range(40 - len(inputs)):\n",
    "                inputs.append( torch.empty((1,360,640), dtype=torch.int8) )\n",
    "            inputs = torch.stack(inputs[:40], 0)\n",
    "            #transform inputs\n",
    "            #print(inputs.size())\n",
    "            #torch.Size([40, 1, 360, 640])\n",
    "            inputs = transform(inputs.to(torch.float)).movedim(0,1)\n",
    "            #print(inputs.size())\n",
    "            \n",
    "\n",
    "            #check if labels exist\n",
    "            label_df = dict_target[int_epnum].query('seg==' + str(int_segnum) + ' & type==\"' + TRAINING_TYPE + '\"' )\n",
    "            if len(label_df) == 0:\n",
    "                print('No labels for ep:' + str(int_epnum) + ' seg:' + str(int_segnum))\n",
    "                continue\n",
    "            labels = torch.squeeze(torch.tensor(label_df[list_datacols].fillna(0).values).to(torch.float))\n",
    "            #normalize labels \n",
    "            if TRAINING_TYPE == 'p':\n",
    "                labels = labels / 600.0\n",
    "            elif TRAINING_TYPE == 'i':\n",
    "                labels = (labels - 80) / 80.0\n",
    "            elif TRAINING_TYPE == 's':\n",
    "                labels = labels\n",
    "            elif TRAINING_TYPE == 'j':\n",
    "                labels = labels\n",
    "            elif TRAINING_TYPE == 'h':\n",
    "                labels = (labels - 10) / 10\n",
    "            \n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = a(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        #if i % 1000 == 0:\n",
    "        #    print(f'Epoch={epoch + 1} Iter={i + 1:5d} Loss={loss.item():.3f}')\n",
    "        #    running_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #define a function that, given a targetdf value, look up the source - if both exists, use the data to train the model\n",
    "\n",
    "#end loop through data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model down for future use / training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have some metric to show "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
