{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg in e:\\python310\\lib\\site-packages (1.4)\n",
      "Requirement already satisfied: av in e:\\python310\\lib\\site-packages (12.0.0)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in e:\\python310\\lib\\site-packages (2.2.2+cu121)\n",
      "Requirement already satisfied: torchvision in e:\\python310\\lib\\site-packages (0.17.2+cu121)\n",
      "Requirement already satisfied: torchaudio in e:\\python310\\lib\\site-packages (2.2.2+cu121)\n",
      "Requirement already satisfied: filelock in e:\\python310\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in e:\\python310\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in e:\\python310\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in e:\\python310\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in e:\\python310\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in e:\\python310\\lib\\site-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: numpy in e:\\python310\\lib\\site-packages (from torchvision) (1.24.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\python310\\lib\\site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\python310\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in e:\\python310\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "#package installs\n",
    "#E:\\Python310\\python.exe -m pip install --upgrade pip\n",
    "#!E:\\Python310\\python.exe -m pip install git+https://github.com/openai/whisper.git soundfile\n",
    "!E:\\Python310\\Scripts\\pip3.exe install ffmpeg av\n",
    "#!E:\\Python310\\python.exe -m pip uninstall torch torchvision torchaudio -y\n",
    "!E:\\Python310\\python.exe -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV_HOSTNAME:JWGamingPC\n",
      "ENV_FOLDER_DATA:E:\\W4732 Computer Vision\\Final Paper Data\\\n",
      "ENV_PRATT:\n"
     ]
    }
   ],
   "source": [
    "#Global variables\n",
    "import socket\n",
    "import os\n",
    "ENV_HOSTNAME = socket.gethostname()\n",
    "print('ENV_HOSTNAME:' + ENV_HOSTNAME)\n",
    "\n",
    "#store defaults for Jacob here:\n",
    "ENV_FOLDER_DATA = 'C:\\\\Users\\\\jakes\\\\Documents\\\\COMS 4732 - Computer Vision\\\\W4732CompVisFinal\\\\Data\\\\'\n",
    "ENV_FOLDER_DATA_PROC = 'C:\\\\Users\\\\jakes\\\\Documents\\\\COMS 4732 - Computer Vision\\\\W4732CompVisFinal\\\\Data_Proc\\\\'\n",
    "ENV_PRATT = ''\n",
    "\n",
    "\n",
    "if ENV_HOSTNAME == 'JWGamingPC':\n",
    "    ENV_FOLDER_DATA = 'E:\\\\W4732 Computer Vision\\\\Final Paper Data\\\\'\n",
    "    ENV_FOLDER_DATA_PROC = 'E:\\\\W4732 Computer Vision\\\\Final Paper Data Proc\\\\'\n",
    "    ENV_PRATT = ''\n",
    "\n",
    "print('ENV_FOLDER_DATA:' + ENV_FOLDER_DATA)\n",
    "print('ENV_PRATT:' + ENV_PRATT)\n",
    "\n",
    "# Create folder structure\n",
    "import os\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'segmentation', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'targetdf', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'pratt', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'eps', exist_ok=True)\n",
    "os.makedirs(ENV_FOLDER_DATA_PROC + 'clips', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Model here\n",
    "# Need to define it up top to prevent overwriting it by mistake\n",
    "import torch\n",
    "\n",
    "class AudGenModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudGenModel, self).__init__()\n",
    "\n",
    "        chanMult = 56\n",
    "        kernel = 4\n",
    "        stride = 2\n",
    "        num_frames = 40\n",
    "        self.relu = torch.nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv3d(in_channels = 1, out_channels = chanMult\n",
    "                                     , kernel_size = (1,360,640) ,stride = stride ,padding = 1 \n",
    "                                     #, depth=num_frames\n",
    "                                    )\n",
    "        self.conv2 = torch.nn.Conv3d(in_channels = chanMult, out_channels = 2 * chanMult\n",
    "                                     , kernel_size = kernel ,stride = stride ,padding = 1\n",
    "                                     #, depth=num_frames\n",
    "                                     )\n",
    "        self.conv3 = torch.nn.Conv3d(in_channels = 2 * chanMult, out_channels = 4 * chanMult\n",
    "                                     , kernel_size = 3 ,stride = stride ,padding = 1\n",
    "                                     #, depth=num_frames\n",
    "                                     )\n",
    "        self.output = torch.nn.Linear(1120, 41)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "\n",
    "        ################################\n",
    "        # Please fill in your code here:\n",
    "        ################################\n",
    "        \n",
    "        x = self.relu(self.conv1(input))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = torch.flatten(x,0) #keep the 1st dim since it's the batch\n",
    "        #x = self.relu(self.hidden(x))\n",
    "        #x = torch.functional.dropout(x,0.25)\n",
    "        x = self.output(x)\n",
    "        out = self.sigmoid(x)\n",
    "\n",
    "        \n",
    "        return out\n",
    "    \n",
    "a = AudGenModel()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "a = a.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 164\u001b[0m\n\u001b[0;32m    162\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# zero the parameter gradients\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m    167\u001b[0m outputs \u001b[38;5;241m=\u001b[39m a(inputs)\n",
      "File \u001b[1;32me:\\Python310\\lib\\site-packages\\torch\\_compile.py:20\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03mThis API should be only used inside torch, external users should still use\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mtorch._dynamo.disable. The main goal of this API is to avoid circular\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03mthe invocation of the decorated function.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Model Training 1 ##\n",
    "import pandas as pd\n",
    "import json\n",
    "import copy\n",
    "import csv\n",
    "import pickle\n",
    "import torchvision\n",
    "import itertools\n",
    "import torch\n",
    "import glob\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "#get number after # but before space afterwards\n",
    "\n",
    "#generate filepaths\n",
    "#filepath_mp4 = ENV_FOLDER_DATA + filesuf_mp4\n",
    "#folderpath_normmp4 = ENV_FOLDER_DATA_PROC + 'norm\\\\' + str_epnum + '\\\\'\n",
    "#os.makedirs(folderpath_normmp4, exist_ok=True)\n",
    "#filepath_normmp4 = folderpath_normmp4 + str_epnum + '.mp4'\n",
    "#folderpath_eps = ENV_FOLDER_DATA_PROC + 'eps\\\\' + str_epnum + '\\\\'\n",
    "#os.makedirs(folderpath_eps, exist_ok=True)\n",
    "#folderpath_clips = ENV_FOLDER_DATA_PROC + 'clips\\\\' + str_epnum + '\\\\'\n",
    "#os.makedirs(folderpath_clips, exist_ok=True)\n",
    "folderpath_clipinfo = ENV_FOLDER_DATA_PROC + 'clipinfo\\\\'\n",
    "os.makedirs(folderpath_clipinfo, exist_ok=True)\n",
    "folderpath_sourcedf = ENV_FOLDER_DATA_PROC + 'sourcedf\\\\'\n",
    "os.makedirs(folderpath_sourcedf, exist_ok=True)\n",
    "#filepath_sourcedf = folderpath_sourcedf + str_epnum + '.pickle'\n",
    "#folderpath_wavsplit = folderpath_eps + 'wavsplit\\\\'\n",
    "#os.makedirs(folderpath_wavsplit, exist_ok=True)\n",
    "#filepath_segmentation = ENV_FOLDER_DATA_PROC + 'segmentation\\\\' + str_epnum + '.psv'\n",
    "folderpath_targetdf = ENV_FOLDER_DATA_PROC + 'targetdf\\\\'\n",
    "folderpath_pratt = ENV_FOLDER_DATA_PROC + 'pratt\\\\' \n",
    "filepath_speakers = ENV_FOLDER_DATA_PROC + 'speakers.json'\n",
    "\n",
    "## get overlap between sourcedf files and targetdf files\n",
    "list_eps = []\n",
    "\n",
    "#note - source is actually a dict of dict of tensors\n",
    "dict_sourcedf = {}\n",
    "list_sourcedf_eps = []\n",
    "for path_pickle in glob.glob(folderpath_sourcedf + '*.pickle'):\n",
    "    dict_temp = {}\n",
    "    dict_temp['path'] = path_pickle\n",
    "    str_basenm = os.path.basename(path_pickle)\n",
    "    dict_temp['basenm'] = str_basenm\n",
    "    dict_temp['str_epnum'] = str_basenm.split('.')[0]\n",
    "    int_epnum = int(dict_temp['str_epnum'])\n",
    "    dict_temp['int_epnum'] = int_epnum\n",
    "    list_sourcedf_eps.append(int_epnum)\n",
    "    dict_sourcedf[int_epnum] = dict_temp\n",
    "\n",
    "#note - target is actually a df\n",
    "dict_targetdf = {}\n",
    "list_targetdf_eps = []\n",
    "for path_pickle in glob.glob(folderpath_targetdf + '*.pickle'):\n",
    "    dict_temp = {}\n",
    "    dict_temp['path'] = path_pickle\n",
    "    str_basenm = os.path.basename(path_pickle)\n",
    "    dict_temp['basenm'] = str_basenm\n",
    "    dict_temp['str_epnum'] = str_basenm.split('.')[0]\n",
    "    int_epnum = int(dict_temp['str_epnum'])\n",
    "    dict_temp['int_epnum'] = int_epnum\n",
    "    list_targetdf_eps.append(int_epnum)\n",
    "    dict_targetdf[int_epnum] = dict_temp\n",
    "\n",
    "#populate matches\n",
    "for src_ep in list_sourcedf_eps:\n",
    "    if src_ep in list_targetdf_eps:\n",
    "        list_eps.append(src_ep)\n",
    "\n",
    "#40 data elements\n",
    "list_datacols = []\n",
    "list_datacols.append('m')\n",
    "for i in range(40):\n",
    "    list_datacols.append('d' + str(i))\n",
    "\n",
    "#define model pieces\n",
    "NUM_EPOCHS = 50\n",
    "TRAINING_TYPE = 'p'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform=torchvision.transforms.Normalize( mean=(255*0.5), std=(255*0.5) )\n",
    "optimizer = torch.optim.Adam(a.parameters(), lr=0.0008, betas=(0.65, 0.999))\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "\n",
    "##Epoch Loop\n",
    "for e in range(NUM_EPOCHS):\n",
    "    #make a copy of the episode list to pop out 2 at a time\n",
    "    list_epocheps = copy.deepcopy(list_eps)\n",
    "    while len(list_epocheps) > 0:\n",
    "        #load two or 3 episodes into memory at a time\n",
    "        list_tempeps = []\n",
    "        for i in range(2):\n",
    "            if len(list_epocheps) > 0:\n",
    "                list_tempeps.append(list_epocheps.pop())\n",
    "        #load pratt files into memory\n",
    "        dict_target = {}\n",
    "        for i,tempeps in enumerate(list_tempeps):\n",
    "            with open(dict_targetdf[tempeps]['path'], 'rb') as file:\n",
    "                dict_target[tempeps] =  pickle.load(file)\n",
    "                    \n",
    "        #load video files into memory\n",
    "        dict_source = {}\n",
    "        for i,tempeps in enumerate(list_tempeps):\n",
    "            with open(dict_sourcedf[tempeps]['path'], 'rb') as file:\n",
    "                dict_temp = pickle.load(file)\n",
    "                dict_source[tempeps] = dict_temp\n",
    "\n",
    "        #create a list of eps / clips and shuffle\n",
    "        listtup_epseg = []\n",
    "        for key,val in dict_source.items():\n",
    "            for key2,val2 in val.items():\n",
    "                tup_epseg = [key,key2]\n",
    "                listtup_epseg.append( tup_epseg)\n",
    "        random.shuffle(tup_epseg)\n",
    "\n",
    "        #loop for data elements\n",
    "        for tup_epseg in listtup_epseg:\n",
    "            int_epnum = tup_epseg[0]\n",
    "            int_segnum = tup_epseg[1]\n",
    "            #feed values into the predict / loss / optimize cycle\n",
    "            inputs = None\n",
    "            labels = None\n",
    "            if int_epnum not in dict_source.keys():\n",
    "                continue\n",
    "            if int_segnum not in dict_source[int_epnum].keys():\n",
    "                continue\n",
    "            inputs = dict_source[int_epnum][int_segnum]\n",
    "            #pad inputs\n",
    "            #for x in range(40 - len()):\n",
    "            #    inputs.append( torch.empty((1,360,640), dtype=torch.int8) )\n",
    "            #inputs = torch.stack(inputs[:40], 0)\n",
    "            #transform inputs\n",
    "            #print(inputs.size())\n",
    "            #torch.Size([40, 1, 360, 640])\n",
    "            inputs = transform(inputs.to(torch.float)).movedim(0,1)\n",
    "            #print(inputs.size())\n",
    "            \n",
    "\n",
    "            #check if labels exist\n",
    "            label_df = dict_target[int_epnum].query('seg==' + str(int_segnum) + ' & type==\"' + TRAINING_TYPE + '\"' )\n",
    "            if len(label_df) == 0:\n",
    "                print('No labels for ep:' + str(int_epnum) + ' seg:' + str(int_segnum))\n",
    "                continue\n",
    "            labels = torch.squeeze(torch.tensor(label_df[list_datacols].fillna(0).values).to(torch.float))\n",
    "            #normalize labels \n",
    "            if TRAINING_TYPE == 'p':\n",
    "                labels = (labels-150) / 150.0\n",
    "            elif TRAINING_TYPE == 'i':\n",
    "                labels = (labels - 80) / 80.0\n",
    "            elif TRAINING_TYPE == 's':\n",
    "                labels = labels\n",
    "            elif TRAINING_TYPE == 'j':\n",
    "                labels = labels\n",
    "            elif TRAINING_TYPE == 'h':\n",
    "                labels = (labels - 10) / 10\n",
    "            \n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = a(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        #if i % 1000 == 0:\n",
    "        #    print(f'Epoch={epoch + 1} Iter={i + 1:5d} Loss={loss.item():.3f}')\n",
    "        #    running_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #define a function that, given a targetdf value, look up the source - if both exists, use the data to train the model\n",
    "\n",
    "#end loop through data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model down for future use / training\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
