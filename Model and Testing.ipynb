{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "import socket\n",
    "import os\n",
    "ENV_HOSTNAME = socket.gethostname()\n",
    "print('ENV_HOSTNAME:' + ENV_HOSTNAME)\n",
    "\n",
    "#store defaults for Jacob here:\n",
    "ENV_FOLDER_DATA = 'C:\\\\Users\\\\jakes\\\\Documents\\\\COMS 4732 - Computer Vision\\\\W4732CompVisFinal\\\\Data\\\\'\n",
    "ENV_FOLDER_DATA_PROC = 'C:\\\\Users\\\\jakes\\\\Documents\\\\COMS 4732 - Computer Vision\\\\W4732CompVisFinal\\\\Data_Proc\\\\'\n",
    "ENV_PRATT = ''\n",
    "\n",
    "\n",
    "if ENV_HOSTNAME == 'JWGamingPC':\n",
    "    ENV_FOLDER_DATA = 'E:\\\\W4732 Computer Vision\\\\Final Paper Data\\\\'\n",
    "    ENV_FOLDER_DATA_PROC = 'E:\\\\W4732 Computer Vision\\\\Final Paper Data Proc\\\\'\n",
    "    ENV_PRATT = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preprocess_video(video_path, desired_fps=10, target_resolution=(640, 360)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame = cv2.resize(frame, target_resolution)\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    frame_interval = int(10 / desired_fps)\n",
    "    sampled_frames = frames[::frame_interval]\n",
    "\n",
    "    video_data = np.array(sampled_frames)\n",
    "    video_data = video_data / 255.0\n",
    "\n",
    "    \n",
    "    # video_data = np.expand_dims(video_data, axis=-1)\n",
    "\n",
    "    if video_data.shape[0] < 40:\n",
    "        # Pad the video data with zeros to make it at least 40 frames long\n",
    "        padding = np.zeros((40 - video_data.shape[0], video_data.shape[1], video_data.shape[2]))\n",
    "        video_data = np.concatenate((video_data, padding), axis=0)\n",
    "    elif video_data.shape[0] > 40:\n",
    "        # Trim the video data to be exactly 40 frames long\n",
    "        video_data = video_data[:40]\n",
    "    \n",
    "    return video_data\n",
    "\n",
    "# Example usage:\n",
    "# video_path = ENV_FOLDER_DATA_PROC + 'clips\\\\501\\\\501_524_528.mp4'\n",
    "# preprocessed_data = preprocess_video(video_path)\n",
    "# print(\"Preprocessed video data shape:\", preprocessed_data.shape)\n",
    "\n",
    "# video_dir = ENV_FOLDER_DATA_PROC + 'clips\\\\595\\\\'\n",
    "\n",
    "# for video in os.listdir(video_dir):\n",
    "#     video_path = video_dir + video\n",
    "#     preprocessed_data = preprocess_video(video_path)\n",
    "#     display_frame = np.vstack(preprocessed_data)\n",
    "\n",
    "#     plt.imshow(display_frame, cmap='gray')\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "#     # # display frames\n",
    "#     # for frame in preprocessed_data:\n",
    "#     #     cv2.imshow('frame', frame)\n",
    "#     # break\n",
    "#     #print(\"Preprocessed video data shape:\", preprocessed_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Data loader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, desired_fps=10, target_resolution=(640, 360), label_type='p'):\n",
    "        self.video_paths = video_paths\n",
    "        self.desired_fps = desired_fps\n",
    "        self.target_resolution = target_resolution\n",
    "        self.label_type = label_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        video_data = preprocess_video(video_path, self.desired_fps, self.target_resolution)\n",
    "        video_data = np.expand_dims(video_data, axis=0)\n",
    "\n",
    "        features = torch.tensor(video_data, dtype=torch.float32)\n",
    "\n",
    "        episode_number = video_path.split('\\\\')[-2]\n",
    "        seg_number = video_path.split('_seg')[-1].split('.')[0]\n",
    "        pickle_path = ENV_FOLDER_DATA_PROC + 'targetdf\\\\' + episode_number + '.pickle'\n",
    "        target_df = pd.read_pickle(pickle_path)\n",
    "\n",
    "        #find row where seg number is equal to seg number and type is equal to label type\n",
    "        target_row = target_df[(target_df['seg'] == int(seg_number)) & (target_df['type'] == self.label_type)]\n",
    "\n",
    "        #extract the data from columns 3:43\n",
    "\n",
    "        audio_data = target_row.iloc[:, 4:45].values\n",
    "\n",
    "        labels = torch.tensor(audio_data.astype(float).flatten())\n",
    "        \n",
    "        labels = torch.nan_to_num(labels)\n",
    "\n",
    "        labels = labels / 600.0\n",
    "        \n",
    "        # if len(features) != 40 or len(labels) != 41:\n",
    "        #     #print(episode_number, seg_number, len(features), len(labels))\n",
    "        #     #write to ENV_FOLDER_DATA_PROC + 'invalid_vid_paths.txt'\n",
    "        #     with open(ENV_FOLDER_DATA_PROC + 'invalid_vid_paths.txt', 'a') as f:\n",
    "        #         f.write(video_path + '\\n')\n",
    "            \n",
    "        #     labels = torch.zeros(41)\n",
    "\n",
    "        return features, labels\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "clips_dir = ENV_FOLDER_DATA_PROC + 'clips\\\\'\n",
    "# Example usage\n",
    "video_paths = []\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(clips_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".mp4\"):\n",
    "            video_paths.append(os.path.join(root, file))\n",
    "\n",
    "\n",
    "training_paths = video_paths[:int(len(video_paths) * 0.8)]\n",
    "test_paths = video_paths[int(len(video_paths) * 0.8):]\n",
    "\n",
    "training_dataset = VideoDataset(training_paths)\n",
    "test_dataset = VideoDataset(test_paths)\n",
    "\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=12, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "# dataset = VideoDataset(video_paths)\n",
    "# # discard all data points that are None,None\n",
    "# #dataset = [data for data in dataset if data != (None, None)]\n",
    "# dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "print(len(training_dataset))\n",
    "# Iterate over batches\n",
    "for batch_idx, (data, labels) in enumerate(training_dataloader):\n",
    "    # 'data' will have shape (batch_size, 1, 40, 640, 360)\n",
    "    # We can extract the first frame (index 0 along the frame dimension)\n",
    "    first_frame_batch = data[:, 0, 0]\n",
    "\n",
    "    # Iterate through each sample in the batch\n",
    "    for i in range(len(first_frame_batch)):\n",
    "        # Get the first frame of the sample and its corresponding label\n",
    "        first_frame = first_frame_batch[i]\n",
    "        label = labels[i]\n",
    "\n",
    "        # Convert the torch tensor to a numpy array and transpose dimensions\n",
    "        # Assuming the tensor is in (channel, depth, height, width) format\n",
    "        first_frame_np = first_frame.numpy()\n",
    "\n",
    "        # Display the first frame using matplotlib\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(first_frame_np.squeeze(), cmap='gray')  # Squeeze out the channel dimension\n",
    "        plt.title(f'Batch {batch_idx + 1}, Sample {i + 1}, Label: {label}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # Only display the first batch\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def move_files_to_folder(file_list_path, destination_folder):\n",
    "    # Create the destination folder if it doesn't exist\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    # Read the list of file paths from the text file\n",
    "    with open(file_list_path, 'r') as file:\n",
    "        file_paths = file.readlines()\n",
    "\n",
    "    # Remove leading/trailing whitespaces and newline characters\n",
    "    file_paths = [path.strip() for path in file_paths]\n",
    "\n",
    "    # Move each file to the destination folder\n",
    "    for file_path in file_paths:\n",
    "        if os.path.exists(file_path):  # Check if file exists\n",
    "            file_name = os.path.basename(file_path)\n",
    "            destination_path = os.path.join(destination_folder, file_name)\n",
    "            shutil.move(file_path, destination_path)\n",
    "            print(f\"Moved '{file_path}' to '{destination_path}'\")\n",
    "        else:\n",
    "            print(f\"File '{file_path}' not found.\")\n",
    "\n",
    "# Example usage:\n",
    "# file_list_path = ENV_FOLDER_DATA_PROC + 'invalid_vid_paths.txt'  # Path to the text file containing the list of file paths\n",
    "# destination_folder = ENV_FOLDER_DATA_PROC + 'invalid_clips\\\\'  # Destination folder to move the files to\n",
    "# move_files_to_folder(file_list_path, destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to test hypotheses\n",
    "\n",
    "# 2 Layers of 3D CONV\n",
    "# 3 Layers of 2D CONV\n",
    "# Max pooling after each layer\n",
    "# Batch normalization after each layer\n",
    "# Flatten\n",
    "# 3 Fully connected layers\n",
    "\n",
    "# Leaky ReLU activation function\n",
    "\n",
    "# adam optimizer\n",
    "# Loss Function: Cross Entropy\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VideoConvNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=41):\n",
    "        super().__init__()\n",
    "        # TODO: Initialize network layers\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=in_channels, out_channels=8, kernel_size=6, padding=6, stride=3),\n",
    "            nn.BatchNorm3d(8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d(kernel_size=4, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=8, out_channels=32, kernel_size=5, padding=5, stride=3),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d(kernel_size=4, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=32, out_channels=64, kernel_size=4, padding=4, stride=2),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d(kernel_size=3, stride=2))\n",
    "        # self.layer4 = nn.Sequential(\n",
    "        #     nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "        #     nn.BatchNorm2d(128),\n",
    "        #     nn.LeakyReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(960, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(128, num_classes))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass with using the layers defined above\n",
    "        #       and the proper activation functions\n",
    "        #print(\"input: \", x.shape)\n",
    "        x = self.layer1(x)\n",
    "        #print(\"layer1: \", x.shape)\n",
    "        x = self.layer2(x)\n",
    "        #print(\"layer2: \", x.shape) \n",
    "        x = self.layer3(x)\n",
    "        #print(\"layer3: \", x.shape)\n",
    "        #x = self.layer4(x)\n",
    "        #print(\"layer4: \", x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(\"flatten: \", x.shape)\n",
    "        x = self.fc1(x)\n",
    "        #print(\"fc1: \", x.shape)\n",
    "        x = self.fc2(x)\n",
    "        #print(\"fc2: \", x.shape)\n",
    "        x = self.fc3(x)\n",
    "        #print(\"fc3: \", x.shape)\n",
    "        return x\n",
    "\n",
    "vidNet = VideoConvNet()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming you have a CUDA-enabled GPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Move the model to the GPU\n",
    "# vidNet.to(device)\n",
    "\n",
    "# # Loss function and optimizer\n",
    "# import torch.optim as optim\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(vidNet.parameters(), lr=0.01)\n",
    "\n",
    "# # Training the network\n",
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(training_dataloader, 0):\n",
    "#         inputs, labels = data\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = vidNet(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 10 == 9:\n",
    "#             print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 10}\")\n",
    "#             running_loss = 0.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vidNet.parameters(), lr=0.001)\n",
    "def train_network(net, n_epochs=2, dataloader=training_dataloader, device=device):\n",
    "    net.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        for i, data in enumerate(training_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "                        # After the backward pass\n",
    "            # for name, param in vidNet.named_parameters():\n",
    "            #     if param.requires_grad and param.grad is not None:\n",
    "            #         print(f'Parameter: {name}, Gradient norm: {param.grad.norm()}')\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            if i % 10 == 0:\n",
    "                print(f'Epoch={epoch + 1} Iter={i + 1:5d} Loss={loss.item():.3f}')\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_network(vidNet, n_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(vidNet.state_dict(), \"pitchVidNetV1.pt\")\n",
    "# torch.save(vidNet.state_dict(), \"pitchVidNetV2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VideoConvNet()\n",
    "# model.load_state_dict(torch.load(\"pitchVidNetV1.pt\"))\n",
    "# model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on the test dataset\n",
    "\n",
    "dataiter = iter(test_dataloader)\n",
    "\n",
    "running_loss = 0.0\n",
    "running_count = 0\n",
    "\n",
    "# Set model to evaluation mode\n",
    "\n",
    "def test_network(model, dataiter):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataiter:\n",
    "            # Move data and labels to the device (GPU)\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(data)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, labels)  # Use appropriate loss function\n",
    "\n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item()\n",
    "            running_count += 1\n",
    "            #print(f'Batch Loss: {loss.item():.6f}')\n",
    "\n",
    "    # Calculate average loss\n",
    "    average_loss = running_loss / running_count\n",
    "\n",
    "    print(f'Average Loss: {average_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "intensity = 'i'\n",
    "harmonics = 'h'\n",
    "jitter = 'j'\n",
    "shimmer = 's'\n",
    "pitch = 'p'\n",
    "\n",
    "# Test the model on the test dataset\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(video_paths)\n",
    "\n",
    "training_intestity_paths = video_paths[:int(len(video_paths) * 0.8)]\n",
    "test_intensity_paths = video_paths[int(len(video_paths) * 0.8):]\n",
    "\n",
    "training_intensity_dataset = VideoDataset(training_intestity_paths, label_type=intensity)\n",
    "test_intensity_dataset = VideoDataset(test_intensity_paths, label_type=intensity)\n",
    "\n",
    "training_intensity_dataloader = DataLoader(training_intensity_dataset, batch_size=12, shuffle=True)\n",
    "test_intensity_dataloader = DataLoader(test_intensity_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "intensity_model = VideoConvNet()\n",
    "\n",
    "train_network(intensity_model, n_epochs=1, dataloader=training_intensity_dataloader)\n",
    "\n",
    "torch.save(intensity_model.state_dict(), \"intensityVidNetV1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(video_paths)\n",
    "\n",
    "training_pitch_paths = video_paths[:int(len(video_paths) * 0.8)]\n",
    "test_pitch_paths = video_paths[int(len(video_paths) * 0.8):]\n",
    "\n",
    "training_pitch_dataset = VideoDataset(training_pitch_paths, label_type=pitch)\n",
    "test_pitch_dataset = VideoDataset(test_pitch_paths, label_type=pitch)\n",
    "\n",
    "training_pitch_dataloader = DataLoader(training_pitch_dataset, batch_size=12, shuffle=True)\n",
    "test_pitch_dataloader = DataLoader(test_pitch_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "pitch_model = VideoConvNet()\n",
    "\n",
    "train_network(pitch_model, n_epochs=1, dataloader=training_pitch_dataloader)\n",
    "\n",
    "torch.save(pitch_model.state_dict(), \"pitchVidNetV1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(video_paths)\n",
    "\n",
    "training_harmonics_paths = video_paths[:int(len(video_paths) * 0.8)]\n",
    "test_harmonics_paths = video_paths[int(len(video_paths) * 0.8):]\n",
    "\n",
    "training_harmonics_dataset = VideoDataset(training_harmonics_paths, label_type=harmonics)\n",
    "test_harmonics_dataset = VideoDataset(test_harmonics_paths, label_type=harmonics)\n",
    "\n",
    "training_harmonics_dataloader = DataLoader(training_harmonics_dataset, batch_size=12, shuffle=True)\n",
    "test_harmonics_dataloader = DataLoader(test_harmonics_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "harmonics_model = VideoConvNet()\n",
    "\n",
    "train_network(harmonics_model, n_epochs=1, dataloader=training_harmonics_dataloader)\n",
    "\n",
    "torch.save(harmonics_model.state_dict(), \"harmonicsVidNetV1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(video_paths)\n",
    "\n",
    "training_jitter_paths = video_paths[:int(len(video_paths) * 0.8)]\n",
    "test_jitter_paths = video_paths[int(len(video_paths) * 0.8):]\n",
    "\n",
    "training_jitter_dataset = VideoDataset(training_jitter_paths, label_type=jitter)\n",
    "test_jitter_dataset = VideoDataset(test_jitter_paths, label_type=jitter)\n",
    "\n",
    "training_jitter_dataloader = DataLoader(training_jitter_dataset, batch_size=12, shuffle=True)\n",
    "test_jitter_dataloader = DataLoader(test_jitter_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "jitter_model = VideoConvNet()\n",
    "\n",
    "train_network(jitter_model, n_epochs=1, dataloader=training_jitter_dataloader)\n",
    "\n",
    "torch.save(jitter_model.state_dict(), \"jitterVidNetV1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(video_paths)\n",
    "\n",
    "training_shimmer_paths = video_paths[:int(len(video_paths) * 0.8)]\n",
    "test_shimmer_paths = video_paths[int(len(video_paths) * 0.8):]\n",
    "\n",
    "training_shimmer_dataset = VideoDataset(training_shimmer_paths, label_type=shimmer)\n",
    "test_shimmer_dataset = VideoDataset(test_shimmer_paths, label_type=shimmer)\n",
    "\n",
    "training_shimmer_dataloader = DataLoader(training_shimmer_dataset, batch_size=12, shuffle=True)\n",
    "test_shimmer_dataloader = DataLoader(test_shimmer_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "shimmer_model = VideoConvNet()\n",
    "\n",
    "train_network(shimmer_model, n_epochs=1, dataloader=training_shimmer_dataloader)\n",
    "\n",
    "torch.save(shimmer_model.state_dict(), \"shimmerVidNetV1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print rows of pickle\n",
    "\n",
    "video_path = ENV_FOLDER_DATA_PROC + 'clips\\\\595\\\\595_seg1434.mp4'\n",
    "#video_path = ENV_FOLDER_DATA_PROC + 'clips\\\\542\\\\542_seg1963.mp4'\n",
    "\n",
    "label_type = 'p'    \n",
    "episode_number = video_path.split('\\\\')[-2]\n",
    "seg_number = video_path.split('_seg')[-1].split('.')[0]\n",
    "pickle_path = ENV_FOLDER_DATA_PROC + 'targetdf\\\\' + episode_number + '.pickle'\n",
    "target_df = pd.read_pickle(pickle_path)\n",
    "\n",
    "#find row where seg number is equal to seg number and type is equal to label type\n",
    "#print(target_df)\n",
    "target_row = target_df[(target_df['seg'] == int(seg_number)) & (target_df['type'] == label_type)]\n",
    "#print(target_row)\n",
    "\n",
    "#extract the data from columns 3:43\n",
    "\n",
    "audio_data = target_row.iloc[:, 4:].values\n",
    "\n",
    "\n",
    "# print(episode_number,seg_number, pickle_path, label_type)\n",
    "print(audio_data)\n",
    "\n",
    "\n",
    "#replace all nan values with 0\n",
    "\n",
    "\n",
    "labels = torch.tensor(audio_data.astype(float).flatten())\n",
    "\n",
    "# print(audio_data.shape)\n",
    "# print(audio_data.flatten())\n",
    "# print(audio_data.flatten().shape)\n",
    "\n",
    "labels = torch.nan_to_num(labels)\n",
    "\n",
    "print(labels)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
